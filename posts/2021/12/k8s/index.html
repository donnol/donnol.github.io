<!doctype html><html lang=en dir=ltr>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content="What #  docker 带来容器之风，以致容器多不胜数。如何编排和管理众多容器，使得它们同心协力办好事情，即成为了当下最大的课题。
为此，k8s 应运而生。
容器，通讯，存储，配置。
Why #  为编排和管理数量众多的容器。
How #  Install #  k8s: 集群搭建所需资源 #   One or more machines running one of:
 Ubuntu 16.04+
Debian 9+
CentOS 7+
Red Hat Enterprise Linux (RHEL) 7+
Fedora 25+
HypriotOS v1.0.1+
Flatcar Container Linux (tested with 2512.3.0)
 2 GB or more of RAM per machine (any less will leave little room for your apps).">
<meta name=theme-color content="#FFFFFF">
<meta name=color-scheme content="light dark"><meta property="og:title" content="k8s">
<meta property="og:description" content="What #  docker 带来容器之风，以致容器多不胜数。如何编排和管理众多容器，使得它们同心协力办好事情，即成为了当下最大的课题。
为此，k8s 应运而生。
容器，通讯，存储，配置。
Why #  为编排和管理数量众多的容器。
How #  Install #  k8s: 集群搭建所需资源 #   One or more machines running one of:
 Ubuntu 16.04+
Debian 9+
CentOS 7+
Red Hat Enterprise Linux (RHEL) 7+
Fedora 25+
HypriotOS v1.0.1+
Flatcar Container Linux (tested with 2512.3.0)
 2 GB or more of RAM per machine (any less will leave little room for your apps).">
<meta property="og:type" content="article">
<meta property="og:url" content="https://donnol.github.io/posts/2021/12/k8s/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-12-10T00:00:00+00:00">
<meta property="article:modified_time" content="2021-12-10T00:00:00+00:00">
<title>k8s | 我的简单博客</title>
<link rel=manifest href=/manifest.json>
<link rel=icon href=/favicon.png type=image/x-icon>
<link rel=stylesheet href=/book.min.09a284f5a03730d86dde350d2c062b1514e48fdfebba7763941c0674ef65748f.css integrity="sha256-CaKE9aA3MNht3jUNLAYrFRTkj9/rundjlBwGdO9ldI8=" crossorigin=anonymous>
<script defer src=/flexsearch.min.js></script>
<script defer src=/en.search.min.40f65924d39a6b3e3712d7fb7ad191d2a5e94bc43a7c9ff266b09afd58737064.js integrity="sha256-QPZZJNOaaz43Etf7etGR0qXpS8Q6fJ/yZrCa/VhzcGQ=" crossorigin=anonymous></script>
<script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script>
</head>
<body dir=ltr>
<input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control>
<main class="container flex">
<aside class=book-menu>
<div class=book-menu-content>
<nav>
<h2 class=book-brand>
<a class="flex align-center" href=/><span>我的简单博客</span>
</a>
</h2>
<div class=book-search>
<input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/>
<div class="book-search-spinner hidden"></div>
<ul id=book-search-results></ul>
</div>
<ul>
<li>
<input type=checkbox id=section-c8e9ae7896ddc3e931652e7edbcbefe2 class=toggle>
<label for=section-c8e9ae7896ddc3e931652e7edbcbefe2 class="flex justify-between">
<a role=button>2022</a>
</label>
<ul>
<li>
<input type=checkbox id=section-935ca4c1d23ab780a0809b4bc7c413bf class=toggle>
<label for=section-935ca4c1d23ab780a0809b4bc7c413bf class="flex justify-between">
<a role=button>02</a>
</label>
<ul>
<li>
<a href=https://donnol.github.io/posts/2022/02/distractions/>杂念</a>
</li>
</ul>
</li>
<li>
<input type=checkbox id=section-a7c5705090b10b47e082065e722e7e5c class=toggle>
<label for=section-a7c5705090b10b47e082065e722e7e5c class="flex justify-between">
<a role=button>01</a>
</label>
<ul>
<li>
<a href=https://donnol.github.io/posts/2022/01/rust_safe/>Rust与安全</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2022/01/wasmtime/>wasm运行时wasmtime</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2022/01/container_encrypt/>容器镜像加密</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2022/01/smart_contract/>智能合约</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2022/01/learn_go_fast/>Go快速入门</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2022/01/consensus/>consensus</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2022/01/red_black_tree/>红黑树</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2022/01/rust_commonly_used_crate/>Rust常用库</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2022/01/cache/>缓存和数据库如何保持一致</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<input type=checkbox id=section-71121fe02c394c786aa5fdaffc9e5412 class=toggle checked>
<label for=section-71121fe02c394c786aa5fdaffc9e5412 class="flex justify-between">
<a role=button>2021</a>
</label>
<ul>
<li>
<input type=checkbox id=section-393d931b698132759df1a7089df0fcc8 class=toggle checked>
<label for=section-393d931b698132759df1a7089df0fcc8 class="flex justify-between">
<a role=button>12</a>
</label>
<ul>
<li>
<a href=https://donnol.github.io/posts/2021/12/ebpf/>ebpf</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/time_wenzi/>时间和文字</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/etcd/>etcd</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/vscode-go-module/>vscode-go在go.mod在非根目录情况下失效的问题</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/mqtt/>mqtt</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/redis_sds/>redis sds</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/k8s/ class=active>k8s</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/ddia/>数据密集型应用设计</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/burn_cpu_use_golang/>burn cpu use golang</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/docker_compose_extra_host/>docker compose使用extra host让容器访问主机服务</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/dbeaver/>数据库管理工具之dbeaver</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/domain/>Domain-oriented development</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/12/github_action_deploy_hugo_blog/>github action deploy hugo blog</a>
</li>
</ul>
</li>
<li>
<input type=checkbox id=section-b59662e33ca9ad77b7d385cbee7785fe class=toggle>
<label for=section-b59662e33ca9ad77b7d385cbee7785fe class="flex justify-between">
<a role=button>07</a>
</label>
<ul>
<li>
<a href=https://donnol.github.io/posts/2021/07/linux-epoll/>linux epoll</a>
</li>
</ul>
</li>
<li>
<input type=checkbox id=section-3b66512bb507cbdaa9cef9549f229fc1 class=toggle>
<label for=section-3b66512bb507cbdaa9cef9549f229fc1 class="flex justify-between">
<a role=button>01</a>
</label>
<ul>
<li>
<a href=https://donnol.github.io/posts/2021/01/proxy-between-layer/>go实现AOP</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/01/hugo-blog/>hugo搭建博客</a>
</li>
<li>
<a href=https://donnol.github.io/posts/2021/01/pstree/>pstree进程树及说明</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<input type=checkbox id=section-6f89cc6c76d5375fc02e3f7e596280b9 class=toggle>
<label for=section-6f89cc6c76d5375fc02e3f7e596280b9 class="flex justify-between">
<a role=button>2020</a>
</label>
<ul>
<li>
<input type=checkbox id=section-03db05476b76e7f4afe06dfa3e1b675b class=toggle>
<label for=section-03db05476b76e7f4afe06dfa3e1b675b class="flex justify-between">
<a role=button>12</a>
</label>
<ul>
<li>
<a href=https://donnol.github.io/posts/2020/12/go-ctx/>go ctx</a>
</li>
</ul>
</li>
</ul>
</li>
<li>
<a href=https://donnol.github.io/posts/aboutme/about-me/>关于我</a>
</li>
</ul>
<ul>
<li>
<a href=https://github.com/donnol/blog target=_blank rel=noopener>
Github
</a>
</li>
<li>
<a href=https://themes.gohugo.io/hugo-book/ target=_blank rel=noopener>
Hugo Themes
</a>
</li>
</ul>
</nav>
<script>(function(){var a=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")})()</script>
</div>
</aside>
<div class=book-page>
<header class=book-header>
<div class="flex align-center justify-between">
<label for=menu-control>
<img src=/svg/menu.svg class=book-icon alt=Menu>
</label>
<strong>k8s</strong>
<label for=toc-control>
<img src=/svg/toc.svg class=book-icon alt="Table of Contents">
</label>
</div>
<aside class="hidden clearfix">
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#what>What</a></li>
<li><a href=#why>Why</a></li>
<li><a href=#how>How</a>
<ul>
<li><a href=#install>Install</a></li>
<li><a href=#k8s-初始化集群>k8s: 初始化集群</a></li>
</ul>
</li>
<li><a href=#kube-flannelyml>kube-flannel.yml</a>
<ul>
<li><a href=#kubeadm_joinsh>kubeadm_join.sh</a></li>
<li><a href=#一些问题>一些问题</a></li>
<li><a href=#deploy>Deploy</a></li>
</ul>
</li>
<li><a href=#list-watch-模式>list-watch 模式</a></li>
<li><a href=#调试>调试</a>
<ul>
<li><a href=#使用容器-exec-进行调试>使用容器 exec 进行调试</a></li>
<li><a href=#尝试>尝试</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</aside>
</header>
<article class=markdown>
<h1>
<a href=/posts/2021/12/k8s/>k8s</a>
</h1>
<h5>December 10, 2021</h5>
<div>
<a href=/categories/k8s/>k8s</a>
</div>
<div>
<a href=/tags/k8s/>k8s</a>
</div>
<h2 id=what>
What
<a class=anchor href=#what>#</a>
</h2>
<p>docker 带来容器之风，以致容器多不胜数。如何编排和管理众多容器，使得它们同心协力办好事情，即成为了当下最大的课题。</p>
<p>为此，k8s 应运而生。</p>
<p>容器，通讯，存储，配置。</p>
<h2 id=why>
Why
<a class=anchor href=#why>#</a>
</h2>
<p>为编排和管理数量众多的容器。</p>
<h2 id=how>
How
<a class=anchor href=#how>#</a>
</h2>
<h3 id=install>
Install
<a class=anchor href=#install>#</a>
</h3>
<h4 id=k8s-集群搭建所需资源>
k8s: 集群搭建所需资源
<a class=anchor href=#k8s-%e9%9b%86%e7%be%a4%e6%90%ad%e5%bb%ba%e6%89%80%e9%9c%80%e8%b5%84%e6%ba%90>#</a>
</h4>
<blockquote>
<p>One or more machines running one of:</p>
<blockquote>
<p>Ubuntu 16.04+</p>
<p>Debian 9+</p>
<p>CentOS 7+</p>
<p>Red Hat Enterprise Linux (RHEL) 7+</p>
<p>Fedora 25+</p>
<p>HypriotOS v1.0.1+</p>
<p>Flatcar Container Linux (tested with 2512.3.0)</p>
</blockquote>
<p>2 GB or more of RAM per machine (any less will leave little room for your apps).</p>
<p>2 CPUs or more.</p>
<p>Full network connectivity between all machines in the cluster (public or private network is fine).</p>
<p>Unique hostname, MAC address, and product_uuid for every node. See here for more details.</p>
<p>Certain ports are open on your machines. See here for more details.</p>
<p>Swap disabled. You MUST disable swap in order for the kubelet to work properly.</p>
</blockquote>
<p>
<a href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>参考官方文档</a></p>
<h4 id=本地机器起多个虚拟机搭建>
本地机器起多个虚拟机搭建
<a class=anchor href=#%e6%9c%ac%e5%9c%b0%e6%9c%ba%e5%99%a8%e8%b5%b7%e5%a4%9a%e4%b8%aa%e8%99%9a%e6%8b%9f%e6%9c%ba%e6%90%ad%e5%bb%ba>#</a>
</h4>
<h4 id=使用-virtualbox-创建三台虚拟机>
使用 virtualBox 创建三台虚拟机
<a class=anchor href=#%e4%bd%bf%e7%94%a8-virtualbox-%e5%88%9b%e5%bb%ba%e4%b8%89%e5%8f%b0%e8%99%9a%e6%8b%9f%e6%9c%ba>#</a>
</h4>
<p>1.用 NAT 和 host only 网络模式</p>
<blockquote>
<p>virtualBox 安装比较简单，不再介绍，GUI 工具用起来也很方便，这部分只介绍我认为需要提示的部分。</p>
<p><strong>内存推荐 2048M, CPU 推荐 2 个</strong></p>
<p>默认只有一个 NAT 适配器，添加一个 <strong>Host-Only Adapter</strong>。NAT 适配器是虚拟机用来访问互联网的，Host-Only 适配器是用来虚拟机之间通信的。</p>
<p>以 Normal Start 方式启动虚拟机安装完系统以后，因为是 server 版镜像，所以没有图形界面，直接使用用户名密码登录即可。</p>
<p>修改配置，<strong>enp0s8 使用静态 IP</strong>。配置请参考 SSH between Mac OS X host and Virtual Box guest。注意配置时将其中的网络接口名改成你自己的 Host-Only Adapter 对应的接口。</p>
<p>一台虚拟机创建完成以后可以使用 clone 方法复制出两台节点出来，注意 clone 时为新机器的网卡重新初始化 MAC 地址。</p>
<p>三台虚拟机的静态 IP 都配置好以后就可以使用 ssh 在本地主机的终端上操作三台虚机了。虚机使用 Headless Start 模式启动</p>
</blockquote>
<p>
<a href=https://github.com/c-rainstorm/blog/blob/master/devops/%E6%9C%AC%E6%9C%BA%E6%90%AD%E5%BB%BA%E4%B8%89%E8%8A%82%E7%82%B9k8s%E9%9B%86%E7%BE%A4.md>参照</a></p>
<p>
<a href=http://www.zchengjoey.com/posts/Ubuntu1604%E6%90%AD%E5%BB%BAk8s%E9%9B%86%E7%BE%A4%28%E9%99%84%E5%B8%A6docker%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86%29/>参照 2</a></p>
<p>2.用桥接网络模式</p>
<p>vm: 虚拟机上安装 k8s</p>
<p>先添加 key：<code>https_proxy=http://192.168.56.1:51837 curl -s -v https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -</code></p>
<p>然后添加 source：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>cat <span style=color:#e6db74>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
</span><span style=color:#e6db74>deb https://apt.kubernetes.io/ kubernetes-xenial main
</span><span style=color:#e6db74>EOF</span>
</code></pre></div><p>最后更新：<code>sudo apt -o Acquire::http::proxy="http://192.168.56.1:51837" update</code></p>
<p>kubeadm</p>
<p>安装：<code>sudo apt -o Acquire::https::proxy="http://192.168.56.1:51837" install -y kubeadm</code></p>
<p>kubelet</p>
<p>安装：<code>sudo apt -o Acquire::https::proxy="http://192.168.56.1:51837" install -y kubelet</code></p>
<p>kubectl</p>
<p>安装：<code>sudo apt -o Acquire::https::proxy="http://192.168.56.1:51837" install -y kubectl</code></p>
<p>docker</p>
<p>
<a href=https://kubernetes.io/zh/docs/setup/production-environment/container-runtimes/#docker>参照</a></p>
<p>从上面的参照可以看到除了 docker 之外，还可以选择其它运行时。</p>
<p>k8s: 三个工具</p>
<p>kubeadm</p>
<p>作用：用来<strong>初始化集群</strong>的指令。</p>
<p>使用：</p>
<blockquote>
<p>在一台机器上执行<code>kubeadm init</code>，初始化集群，该机器作为集群 master；初始化成功后会返回<code>&lt;arguments-returned-from-init></code>，作为其它机器加入该集群的参数 。</p>
<p>在另一台机器上执行<code>kubeadm join &lt;arguments-returned-from-init></code>。</p>
<p>如果想添加更多机器，请重复<code>kubeadm join</code>指令。</p>
</blockquote>
<p>kubelet</p>
<p>作用：在集群中的每个节点上用来<strong>启动 Pod 和容器</strong>等。</p>
<p>在每个节点上运行的节点代理。它可以向 apiserver 注册节点。</p>
<blockquote>
<p>The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object
that describes a pod.</p>
<p>&ndash; kubelet 在一系列 pod 规范里工作。一个 pod 规范是一个描述 pod 的 yaml 或 json 对象。</p>
</blockquote>
<p>kubectl</p>
<p>作用：用来<strong>与集群通信</strong>的命令行工具。</p>
<p>创建资源，暴露服务。</p>
<p>
<a href=https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>参照</a></p>
<h3 id=k8s-初始化集群>
k8s: 初始化集群
<a class=anchor href=#k8s-%e5%88%9d%e5%a7%8b%e5%8c%96%e9%9b%86%e7%be%a4>#</a>
</h3>
<h4 id=关闭-swap-交换空间>
关闭 swap 交换空间
<a class=anchor href=#%e5%85%b3%e9%97%ad-swap-%e4%ba%a4%e6%8d%a2%e7%a9%ba%e9%97%b4>#</a>
</h4>
<p>执行命令：<code>sudo swapoff -a</code>，并将文件<code>/etc/fstab</code>里关于 swap 的行注释掉，然后重启机器</p>
<p>为什么要关闭呢？</p>
<p>
<a href=https://github.com/kubernetes/kubernetes/issues/53533>issue 上的讨论 1</a></p>
<p>
<a href=https://github.com/kubernetes/kubernetes/issues/7294>issue 上的讨论 2</a></p>
<blockquote>
<p>having swap available has very strange and bad interactions with memory limits</p>
<p>kubernetes 的想法是将实例紧密包装到尽可能接近 100％。 所有的部署应该与 CPU /内存限制固定在一起。 所以如果调度程序发送一个 pod 到一台机器，它不应该使用交换。 设计者不想交换，因为它会减慢速度。</p>
<p>所以关闭 swap 主要是为了性能考虑。</p>
<p>当然为了一些节省资源的场景，比如运行容器数量较多，可添加 kubelet 参数 &ndash;fail-swap-on=false 来解决。</p>
</blockquote>
<h4 id=初始化>
初始化
<a class=anchor href=#%e5%88%9d%e5%a7%8b%e5%8c%96>#</a>
</h4>
<p>先通过命令<code>sudo kubeadm config print init-defaults > init-default.yaml</code>生成默认配置文件。</p>
<blockquote>
<p>在生成的 yaml 配置文件里修改:</p>
<p><code>advertiseAddress: 192.168.9.43</code>，其中的 ip 地址为<code>ip a</code>拿到的地址。</p>
<p>networking:
dnsDomain: cluster.local
podSubnet: 10.46.128.0/21 # 这个 ip 将要用在安装成功后的 pod network 配置里。
serviceSubnet: 192.168.1.0/24</p>
</blockquote>
<p>然后在初始化命令使用该配置文件：<code>sudo kubeadm init --config=init-default.yaml --v=5</code></p>
<p>出现警告：</p>
<blockquote>
<p>[preflight] Running pre-flight checks</p>
<p>[WARNING IsDockerSystemdCheck]: detected &ldquo;cgroupfs&rdquo; as the Docker cgroup driver. The recommended driver is &ldquo;systemd&rdquo;. Please follow the guide at
<a href=https://kubernetes.io/docs/setup/cri/>https://kubernetes.io/docs/setup/cri/</a></p>
</blockquote>
<p>里面说到 cgroup 驱动用了 cgroupfs，而不是 systemd。可参照
<a href=https://kubernetes.io/docs/setup/cri/>官方文档</a>修改设置。</p>
<p>然后会到<code>k8s.gcr.io</code>获取镜像，这时又出现超时错误。应该是墙导致的，需要使用代理或
<a href=https://vqiu.cn/how-to-access-gcr-io/>改用镜像站</a>。</p>
<p>
<a href=https://developer.aliyun.com/article/759310>镜像站也关掉了，怎么办？</a></p>
<p>使用 docker 拉取镜像，然后修改 tag：</p>
<blockquote>
<p>先看所需镜像：<code>sudo kubeadm config images list</code></p>
<blockquote>
<p>k8s.gcr.io/kube-apiserver:v1.20.3</p>
<p>k8s.gcr.io/kube-controller-manager:v1.20.3</p>
<p>k8s.gcr.io/kube-scheduler:v1.20.3</p>
<p>k8s.gcr.io/kube-proxy:v1.20.3</p>
<p>k8s.gcr.io/pause:3.2</p>
<p>k8s.gcr.io/etcd:3.4.13-0</p>
<p>k8s.gcr.io/coredns:1.7.0</p>
</blockquote>
<p>逐个从 docker hub 上搜到并获取镜像：<code>sudo docker pull aiotceo/kube-apiserver:v1.20.3</code></p>
<blockquote>
<p>sudo docker pull aiotceo/kube-apiserver:v1.20.3</p>
<p>sudo docker pull aiotceo/kube-controller-manager:v1.20.3</p>
<p>sudo docker pull aiotceo/kube-scheduler:v1.20.3</p>
<p>sudo docker pull aiotceo/kube-proxy:v1.20.3</p>
<p>sudo docker pull aiotceo/pause:3.2</p>
<p>sudo docker pull bitnami/etcd:3.4.13</p>
<p>sudo docker pull aiotceo/coredns:1.7.0</p>
</blockquote>
<p>修改 tag：<code>sudo docker tag aiotceo/kube-apiserver:v1.20.3 k8s.gcr.io/kube-apiserver:v1.20.3</code></p>
<p>最后删除：<code>sudo docker rmi aiotceo/kube-apiserver:v1.20.3</code></p>
<p>[preflight] You can also perform this action in beforehand using &lsquo;kubeadm config images pull&rsquo;</p>
<p>&ndash; 拉取镜像也可以提前使用<code>kubeadm config images pull</code>完成。</p>
</blockquote>
<p>kubelet 未启动错误：</p>
<blockquote>
<p>[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &ldquo;/etc/kubernetes/manifests&rdquo;. This can take up to 4m0s
I0218 09:29:21.630514 98836 request.go:943] Got a Retry-After 1s response for attempt 1 to
<a href="https://10.0.2.15:6443/healthz?timeout=10s">https://10.0.2.15:6443/healthz?timeout=10s</a> > [kubelet-check] Initial timeout of 40s passed.
I0218 09:29:43.755024 98836 request.go:943] Got a Retry-After 1s response for attempt 1 to
<a href="https://10.0.2.15:6443/healthz?timeout=10s">https://10.0.2.15:6443/healthz?timeout=10s</a>
I0218 09:30:21.758311 98836 request.go:943] Got a Retry-After 1s response for attempt 1 to
<a href="https://10.0.2.15:6443/healthz?timeout=10s">https://10.0.2.15:6443/healthz?timeout=10s</a>
I0218 09:32:24.612682 98836 request.go:943] Got a Retry-After 1s response for attempt 1 to
<a href="https://10.0.2.15:6443/healthz?timeout=10s">https://10.0.2.15:6443/healthz?timeout=10s</a></p>
</blockquote>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>        Unfortunately, an error has occurred:
                timed out waiting <span style=color:#66d9ef>for</span> the condition

        This error is likely caused by:
                - The kubelet is not running
                - The kubelet is unhealthy due to a misconfiguration of the node in some way <span style=color:#f92672>(</span>required cgroups disabled<span style=color:#f92672>)</span>

        If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
                - <span style=color:#e6db74>&#39;systemctl status kubelet&#39;</span>
                - <span style=color:#e6db74>&#39;journalctl -xeu kubelet&#39;</span>

        Additionally, a control plane component may have crashed or exited when started by the container runtime.
        To troubleshoot, list all containers using your preferred container runtimes CLI.

        Here is one example how you may list all Kubernetes containers running in docker:
                - <span style=color:#e6db74>&#39;docker ps -a | grep kube | grep -v pause&#39;</span>
                Once you have found the failing container, you can inspect its logs with:
                - <span style=color:#e6db74>&#39;docker logs CONTAINERID&#39;</span>
</code></pre></div><p>
<a href=https://stackoverflow.com/questions/52119985/kubeadm-init-shows-kubelet-isnt-running-or-healthy>原来是在关闭了 swap 后要重启</a></p>
<p>初始化过程中报错了之后需要重置一下才行：<code>sudo kubeadm reset --v=5</code></p>
<p>否则会报错：</p>
<blockquote>
<p>[preflight] Some fatal errors occurred:</p>
</blockquote>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#f92672>[</span>ERROR Port-10259<span style=color:#f92672>]</span>: Port <span style=color:#ae81ff>10259</span> is in use
<span style=color:#f92672>[</span>ERROR Port-10257<span style=color:#f92672>]</span>: Port <span style=color:#ae81ff>10257</span> is in use
<span style=color:#f92672>[</span>ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml<span style=color:#f92672>]</span>: /etc/kubernetes/manifests/kube-apiserver.yaml already exists
<span style=color:#f92672>[</span>ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml<span style=color:#f92672>]</span>: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists
<span style=color:#f92672>[</span>ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml<span style=color:#f92672>]</span>: /etc/kubernetes/manifests/kube-scheduler.yaml already exists
<span style=color:#f92672>[</span>ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml<span style=color:#f92672>]</span>: /etc/kubernetes/manifests/etcd.yaml already exists
<span style=color:#f92672>[</span>ERROR Port-10250<span style=color:#f92672>]</span>: Port <span style=color:#ae81ff>10250</span> is in use

<span style=color:#f92672>[</span>preflight<span style=color:#f92672>]</span> If you know what you are doing, you can make a check non-fatal with <span style=color:#e6db74>`</span>--ignore-preflight-errors<span style=color:#f92672>=</span>...<span style=color:#e6db74>`</span>
error execution phase preflight
</code></pre></div><p>查看<code>kubelet</code>日志：<code>journalctl -xeu kubelet</code></p>
<p><code>Failed to connect to 10.0.2.15 port 6443: Connection refused</code>:</p>
<p>
<a href=https://zhuanlan.zhihu.com/p/265968760>除了防火墙和 swap，还要关闭 selinux</a></p>
<blockquote>
<p>查看防火墙状态：systemctl status firewalld</p>
<p>临时关闭 selinux：</p>
<blockquote>
<p><code>sudo setenforce 0</code></p>
</blockquote>
<p>永久关闭：</p>
<blockquote>
<p>执行<code>sestatus</code>查看 selinux 状态，需要先安装工具<code>sudo apt install policycoreutils</code></p>
<p>查看后，再去编辑配置文件<code>sudo vim /etc/selinux/config</code>，改为<code>SELINUX=disabled</code></p>
</blockquote>
</blockquote>
<p>
<a href=https://zhuanlan.zhihu.com/p/31398416>需要翻墙的情况下</a></p>
<h4 id=修改-docker-的-cgroup-driver-为-systemd>
修改 docker 的 cgroup driver 为 systemd
<a class=anchor href=#%e4%bf%ae%e6%94%b9-docker-%e7%9a%84-cgroup-driver-%e4%b8%ba-systemd>#</a>
</h4>
<p>为了确保 docker 和 kubelet 的 cgroup driver 一样，需要将 docker 的 cgroup driver 改为 systemd。</p>
<p>查看：<code>sudo docker info</code>，发现<code>Cgroup Driver: cgroupfs</code></p>
<p>修改：<code>sudo vim /etc/docker/daemon.json</code>，添加以下内容：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json>{
  <span style=color:#f92672>&#34;exec-opts&#34;</span>: [<span style=color:#e6db74>&#34;native.cgroupdriver=systemd&#34;</span>]
}
</code></pre></div><p>重启：<code>sudo systemctl daemon-reload</code> <code>sudo systemctl restart docker</code></p>
<p>再次查看 docker info，会看到<code>Cgroup Driver: systemd</code></p>
<h4 id=kubectl>
kubectl
<a class=anchor href=#kubectl>#</a>
</h4>
<p>执行<code>sudo kubelet</code>时出现错误：</p>
<blockquote>
<p>failed to get the kubelet&rsquo;s cgroup: cpu and memory cgroup hierarchy not unified. cpu: /user.slice, memory: /user.slice/user-1000.slice/session-1.scope. Kubelet system container metrics may be missing.</p>
</blockquote>
<p>难道是不能这样直接执行，需要用 systemctl 来执行：<code>sudo systemctl start kubelet.service</code></p>
<p>通过查看<code>sudo cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>配置文件，发现里面有很多 env 设置。</p>
<p>在<code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>配置文件里添加<code>--cgroup-driver=systemd</code>配置。</p>
<p>是不是虚拟机创建时选择的网卡错了（用了 NAT 和 host only），导致上面的这么多问题呢？</p>
<p>
<a href=https://www.cnblogs.com/woncode/p/12206023.html>换成 NAT network 或者桥接试下</a>。</p>
<blockquote>
<p>With bridged networking, Oracle VM VirtualBox uses a device driver on your host system that filters data from your physical network adapter. This driver is therefore called a net filter driver. This enables Oracle VM VirtualBox to intercept data from the physical network and inject data into it, effectively creating a new network interface in software. When a guest is using such a new software interface, it looks to the host system as though the guest were physically connected to the interface using a network cable. The host can send data to the guest through that interface and receive data from it. This means that you can set up routing or bridging between the guest and the rest of your network.</p>
</blockquote>
<p>
<a href=https://www.virtualbox.org/manual/ch06.html>vbox 官方文档关于网卡的介绍</a></p>
<h4 id=成功>
成功
<a class=anchor href=#%e6%88%90%e5%8a%9f>#</a>
</h4>
<p>成功后会有以下信息：</p>
<p>其中需要注意部分信息是需要执行的命令，还有其它节点添加到集群时需要用到的信息。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#f92672>[</span>addons<span style=color:#f92672>]</span> Applied essential addon: CoreDNS
<span style=color:#f92672>[</span>addons<span style=color:#f92672>]</span> Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown <span style=color:#66d9ef>$(</span>id -u<span style=color:#66d9ef>)</span>:<span style=color:#66d9ef>$(</span>id -g<span style=color:#66d9ef>)</span> $HOME/.kube/config

Alternatively, <span style=color:#66d9ef>if</span> you are the root user, you can run:

  export KUBECONFIG<span style=color:#f92672>=</span>/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run <span style=color:#e6db74>&#34;kubectl apply -f [podnetwork].yaml&#34;</span> with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.9.43:6443 --token abcdef.0123456789abcdef <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --discovery-token-ca-cert-hash sha256:5660805073db31916952821a8751ca0ee0644ce4205f616805f8a7f175ff8b33
</code></pre></div><p>
<a href=https://github.com/flannel-io/flannel#deploying-flannel-manually>添加 pod network：</a></p>
<blockquote>
<p>下载配置文件：wget
<a href=https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml>https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</a></p>
<p>编辑配置：修改</p>
<p>net-conf.json: |</p>
<p>{
&ldquo;Network&rdquo;: &ldquo;10.46.128.0/21&rdquo;, # 这个地址要与 kubeadm 指定的配置文件里的 podSubnet 里的 ip 一致
&ldquo;Backend&rdquo;: {
&ldquo;Type&rdquo;: &ldquo;vxlan&rdquo;
}
}</p>
<p>最后执行命令: <code>kubectl apply -f kube-flannel.yml</code></p>
</blockquote>
<p>最后，查看 pods 状态：<code>kubectl get pods --all-namespaces</code></p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>jd@jnode:~$ kubectl get pods --all-namespaces
NAMESPACE     NAME                            READY   STATUS    RESTARTS   AGE
kube-system   coredns-74ff55c5b-f76bb         1/1     Running   <span style=color:#ae81ff>0</span>          5m52s
kube-system   coredns-74ff55c5b-frgkw         1/1     Running   <span style=color:#ae81ff>0</span>          5m52s
kube-system   etcd-jnode                      1/1     Running   <span style=color:#ae81ff>0</span>          6m8s
kube-system   kube-apiserver-jnode            1/1     Running   <span style=color:#ae81ff>0</span>          6m8s
kube-system   kube-controller-manager-jnode   1/1     Running   <span style=color:#ae81ff>0</span>          6m8s
kube-system   kube-flannel-ds-xghcx           1/1     Running   <span style=color:#ae81ff>0</span>          4m4s
kube-system   kube-proxy-nnbbx                1/1     Running   <span style=color:#ae81ff>0</span>          5m52s
kube-system   kube-scheduler-jnode            1/1     Running   <span style=color:#ae81ff>0</span>          6m8s
</code></pre></div><p>全部正常运行，完成。</p>
<h4 id=错误>
错误
<a class=anchor href=#%e9%94%99%e8%af%af>#</a>
</h4>
<p>k8s 的安装过程，可谓是一波三折。下面整理一下遇到的错误：</p>
<p>
<a href=https://github.com/flannel-io/flannel/issues/1344>Error registering network: failed to acquire lease: node &ldquo;nodeName&rdquo; pod cidr not assigned
</a></p>
<blockquote>
<p>确保 Network 的值与 podSubnet 一致即可。</p>
</blockquote>
<h4 id=添加-worker-节点>
添加 worker 节点
<a class=anchor href=#%e6%b7%bb%e5%8a%a0-worker-%e8%8a%82%e7%82%b9>#</a>
</h4>
<p>在初始化完成后的输出信息里有：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.9.43:6443 --token abcdef.0123456789abcdef <span style=color:#ae81ff>\
</span><span style=color:#ae81ff></span>    --discovery-token-ca-cert-hash sha256:5660805073db31916952821a8751ca0ee0644ce4205f616805f8a7f175ff8b33
</code></pre></div><p>这个命令就是 worker 要加入集群时需要执行的命令。</p>
<p>在执行上述命令之前，需要
<a href=https://cloud.tencent.com/developer/news/268376>重新生成 token 和 hash</a>:</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#75715e># 重新生成新的token</span>
kubeadm token create
kubeadm token list

<span style=color:#75715e># 获取ca证书sha256编码hash值，拿到的hash值要记录下来</span>
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed <span style=color:#e6db74>&#39;s/^.* //&#39;</span>

<span style=color:#75715e># 节点加入集群，token和hash值使用上面生成的</span>
kubeadm join ip:6443 --token xxx --discovery-token-ca-cert-hash sha256:xxx
</code></pre></div><p>最后在主节点，执行：<code>kubectl get nodes</code>查看已加入的节点。</p>
<p>重启之后要等一会才会正常：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>$ kubectl get nodes
The connection to the server 192.168.9.43:6443 was refused - did you specify the right host or port?
</code></pre></div><p>如果没有执行<code>export KUBECONFIG=/etc/kubernetes/admin.conf</code>，那么在使用 kubectl 命令时就不能在前面加 sudo 了。</p>
<p>在主节点成功看到 worker node 之后，还需要将主节点里的<code>~/.kube/config</code>文件复制到 worker node 上：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#75715e># 在主节点上使用scp将配置文件复制到worker</span>
scp ~/.kube/config xx@xxx.xxx.xxx.xxx:~/.kube/config
</code></pre></div><p>否则会报错：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>kubectl describe node
The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre></div><p>复制配置到 worker 后，在 worker 节点上执行<code>kubectl describe node</code>，就能看到节点信息。</p>
<p>
<a href=https://stackoverflow.com/questions/63539796/connection-refused-error-on-worker-node-in-kubernetes>参照</a></p>
<h4 id=k8s-初始化配置文件>
k8s: 初始化配置文件
<a class=anchor href=#k8s-%e5%88%9d%e5%a7%8b%e5%8c%96%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6>#</a>
</h4>
<h5 id=init-defaultyaml>
init-default.yaml
<a class=anchor href=#init-defaultyaml>#</a>
</h5>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kubeadm.k8s.io/v1beta2</span>
<span style=color:#f92672>bootstrapTokens</span>:
  - <span style=color:#f92672>groups</span>:
      - <span style=color:#ae81ff>system:bootstrappers:kubeadm:default-node-token</span>
    <span style=color:#f92672>token</span>: <span style=color:#ae81ff>abcdef.0123456789abcdef</span>
    <span style=color:#f92672>ttl</span>: <span style=color:#ae81ff>24h0m0s</span>
    <span style=color:#f92672>usages</span>:
      - <span style=color:#ae81ff>signing</span>
      - <span style=color:#ae81ff>authentication</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>InitConfiguration</span>
<span style=color:#f92672>localAPIEndpoint</span>:
  <span style=color:#f92672>advertiseAddress</span>: <span style=color:#ae81ff>192.168.9.43</span> <span style=color:#75715e># 本机的ip地址，可通过`ip a`获取</span>
  <span style=color:#f92672>bindPort</span>: <span style=color:#ae81ff>6443</span>
<span style=color:#f92672>nodeRegistration</span>:
  <span style=color:#f92672>criSocket</span>: <span style=color:#ae81ff>/var/run/dockershim.sock</span>
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>jnode</span>
  <span style=color:#f92672>taints</span>:
    - <span style=color:#f92672>effect</span>: <span style=color:#ae81ff>NoSchedule</span>
      <span style=color:#f92672>key</span>: <span style=color:#ae81ff>node-role.kubernetes.io/master</span>
---
<span style=color:#f92672>apiServer</span>:
  <span style=color:#f92672>timeoutForControlPlane</span>: <span style=color:#ae81ff>4m0s</span>
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kubeadm.k8s.io/v1beta2</span>
<span style=color:#f92672>certificatesDir</span>: <span style=color:#ae81ff>/etc/kubernetes/pki</span>
<span style=color:#f92672>clusterName</span>: <span style=color:#ae81ff>kubernetes</span>
<span style=color:#f92672>controllerManager</span>: {}
<span style=color:#f92672>dns</span>:
  <span style=color:#f92672>type</span>: <span style=color:#ae81ff>CoreDNS</span>
<span style=color:#f92672>etcd</span>:
  <span style=color:#f92672>local</span>:
    <span style=color:#f92672>dataDir</span>: <span style=color:#ae81ff>/var/lib/etcd</span>
<span style=color:#f92672>imageRepository</span>: <span style=color:#ae81ff>k8s.gcr.io</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterConfiguration</span>
<span style=color:#f92672>kubernetesVersion</span>: <span style=color:#ae81ff>v1.20.0</span>
<span style=color:#f92672>networking</span>:
  <span style=color:#f92672>dnsDomain</span>: <span style=color:#ae81ff>cluster.local</span>
  <span style=color:#f92672>podSubnet</span>: <span style=color:#ae81ff>10.46.128.0</span><span style=color:#ae81ff>/21</span> <span style=color:#75715e># pod subnet值需要与kube-flannel.yml里的网络设置一致</span>
  <span style=color:#f92672>serviceSubnet</span>: <span style=color:#ae81ff>192.168.1.0</span><span style=color:#ae81ff>/24</span> <span style=color:#75715e># 跟本机ip同一网段</span>
<span style=color:#f92672>scheduler</span>: {}
</code></pre></div><h2 id=kube-flannelyml>
kube-flannel.yml
<a class=anchor href=#kube-flannelyml>#</a>
</h2>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml>---
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>policy/v1beta1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PodSecurityPolicy</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>psp.flannel.unprivileged</span>
  <span style=color:#f92672>annotations</span>:
    <span style=color:#f92672>seccomp.security.alpha.kubernetes.io/allowedProfileNames</span>: <span style=color:#ae81ff>docker/default</span>
    <span style=color:#f92672>seccomp.security.alpha.kubernetes.io/defaultProfileName</span>: <span style=color:#ae81ff>docker/default</span>
    <span style=color:#f92672>apparmor.security.beta.kubernetes.io/allowedProfileNames</span>: <span style=color:#ae81ff>runtime/default</span>
    <span style=color:#f92672>apparmor.security.beta.kubernetes.io/defaultProfileName</span>: <span style=color:#ae81ff>runtime/default</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>privileged</span>: <span style=color:#66d9ef>false</span>
  <span style=color:#f92672>volumes</span>:
    - <span style=color:#ae81ff>configMap</span>
    - <span style=color:#ae81ff>secret</span>
    - <span style=color:#ae81ff>emptyDir</span>
    - <span style=color:#ae81ff>hostPath</span>
  <span style=color:#f92672>allowedHostPaths</span>:
    - <span style=color:#f92672>pathPrefix</span>: <span style=color:#e6db74>&#34;/etc/cni/net.d&#34;</span>
    - <span style=color:#f92672>pathPrefix</span>: <span style=color:#e6db74>&#34;/etc/kube-flannel&#34;</span>
    - <span style=color:#f92672>pathPrefix</span>: <span style=color:#e6db74>&#34;/run/flannel&#34;</span>
  <span style=color:#f92672>readOnlyRootFilesystem</span>: <span style=color:#66d9ef>false</span>
  <span style=color:#75715e># Users and groups</span>
  <span style=color:#f92672>runAsUser</span>:
    <span style=color:#f92672>rule</span>: <span style=color:#ae81ff>RunAsAny</span>
  <span style=color:#f92672>supplementalGroups</span>:
    <span style=color:#f92672>rule</span>: <span style=color:#ae81ff>RunAsAny</span>
  <span style=color:#f92672>fsGroup</span>:
    <span style=color:#f92672>rule</span>: <span style=color:#ae81ff>RunAsAny</span>
  <span style=color:#75715e># Privilege Escalation</span>
  <span style=color:#f92672>allowPrivilegeEscalation</span>: <span style=color:#66d9ef>false</span>
  <span style=color:#f92672>defaultAllowPrivilegeEscalation</span>: <span style=color:#66d9ef>false</span>
  <span style=color:#75715e># Capabilities</span>
  <span style=color:#f92672>allowedCapabilities</span>: [<span style=color:#e6db74>&#34;NET_ADMIN&#34;</span>, <span style=color:#e6db74>&#34;NET_RAW&#34;</span>]
  <span style=color:#f92672>defaultAddCapabilities</span>: []
  <span style=color:#f92672>requiredDropCapabilities</span>: []
  <span style=color:#75715e># Host namespaces</span>
  <span style=color:#f92672>hostPID</span>: <span style=color:#66d9ef>false</span>
  <span style=color:#f92672>hostIPC</span>: <span style=color:#66d9ef>false</span>
  <span style=color:#f92672>hostNetwork</span>: <span style=color:#66d9ef>true</span>
  <span style=color:#f92672>hostPorts</span>:
    - <span style=color:#f92672>min</span>: <span style=color:#ae81ff>0</span>
      <span style=color:#f92672>max</span>: <span style=color:#ae81ff>65535</span>
  <span style=color:#75715e># SELinux</span>
  <span style=color:#f92672>seLinux</span>:
    <span style=color:#75715e># SELinux is unused in CaaSP</span>
    <span style=color:#f92672>rule</span>: <span style=color:#e6db74>&#34;RunAsAny&#34;</span>
---
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRole</span>
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io/v1</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel</span>
<span style=color:#f92672>rules</span>:
  - <span style=color:#f92672>apiGroups</span>: [<span style=color:#e6db74>&#34;extensions&#34;</span>]
    <span style=color:#f92672>resources</span>: [<span style=color:#e6db74>&#34;podsecuritypolicies&#34;</span>]
    <span style=color:#f92672>verbs</span>: [<span style=color:#e6db74>&#34;use&#34;</span>]
    <span style=color:#f92672>resourceNames</span>: [<span style=color:#e6db74>&#34;psp.flannel.unprivileged&#34;</span>]
  - <span style=color:#f92672>apiGroups</span>:
      - <span style=color:#e6db74>&#34;&#34;</span>
    <span style=color:#f92672>resources</span>:
      - <span style=color:#ae81ff>pods</span>
    <span style=color:#f92672>verbs</span>:
      - <span style=color:#ae81ff>get</span>
  - <span style=color:#f92672>apiGroups</span>:
      - <span style=color:#e6db74>&#34;&#34;</span>
    <span style=color:#f92672>resources</span>:
      - <span style=color:#ae81ff>nodes</span>
    <span style=color:#f92672>verbs</span>:
      - <span style=color:#ae81ff>list</span>
      - <span style=color:#ae81ff>watch</span>
  - <span style=color:#f92672>apiGroups</span>:
      - <span style=color:#e6db74>&#34;&#34;</span>
    <span style=color:#f92672>resources</span>:
      - <span style=color:#ae81ff>nodes/status</span>
    <span style=color:#f92672>verbs</span>:
      - <span style=color:#ae81ff>patch</span>
---
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRoleBinding</span>
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io/v1</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel</span>
<span style=color:#f92672>roleRef</span>:
  <span style=color:#f92672>apiGroup</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io</span>
  <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRole</span>
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel</span>
<span style=color:#f92672>subjects</span>:
  - <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ServiceAccount</span>
    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel</span>
    <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
---
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ServiceAccount</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
---
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ConfigMap</span>
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-flannel-cfg</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
  <span style=color:#f92672>labels</span>:
    <span style=color:#f92672>tier</span>: <span style=color:#ae81ff>node</span>
    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>flannel</span>
<span style=color:#f92672>data</span>:
  <span style=color:#f92672>cni-conf.json</span>: |<span style=color:#e6db74>
</span><span style=color:#e6db74>    {
</span><span style=color:#e6db74>      &#34;name&#34;: &#34;cbr0&#34;,
</span><span style=color:#e6db74>      &#34;cniVersion&#34;: &#34;0.3.1&#34;,
</span><span style=color:#e6db74>      &#34;plugins&#34;: [
</span><span style=color:#e6db74>        {
</span><span style=color:#e6db74>          &#34;type&#34;: &#34;flannel&#34;,
</span><span style=color:#e6db74>          &#34;delegate&#34;: {
</span><span style=color:#e6db74>            &#34;hairpinMode&#34;: true,
</span><span style=color:#e6db74>            &#34;isDefaultGateway&#34;: true
</span><span style=color:#e6db74>          }
</span><span style=color:#e6db74>        },
</span><span style=color:#e6db74>        {
</span><span style=color:#e6db74>          &#34;type&#34;: &#34;portmap&#34;,
</span><span style=color:#e6db74>          &#34;capabilities&#34;: {
</span><span style=color:#e6db74>            &#34;portMappings&#34;: true
</span><span style=color:#e6db74>          }
</span><span style=color:#e6db74>        }
</span><span style=color:#e6db74>      ]
</span><span style=color:#e6db74>    }</span>    
  <span style=color:#f92672>net-conf.json</span>: |<span style=color:#e6db74>
</span><span style=color:#e6db74>    {
</span><span style=color:#e6db74>      &#34;Network&#34;: &#34;10.46.128.0/21&#34;, # 与init-default.yaml配置文件里的podSubnet值一致
</span><span style=color:#e6db74>      &#34;Backend&#34;: {
</span><span style=color:#e6db74>        &#34;Type&#34;: &#34;vxlan&#34;
</span><span style=color:#e6db74>      }
</span><span style=color:#e6db74>    }</span>    
---
<span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>DaemonSet</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-flannel-ds</span>
  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
  <span style=color:#f92672>labels</span>:
    <span style=color:#f92672>tier</span>: <span style=color:#ae81ff>node</span>
    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>flannel</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>selector</span>:
    <span style=color:#f92672>matchLabels</span>:
      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>flannel</span>
  <span style=color:#f92672>template</span>:
    <span style=color:#f92672>metadata</span>:
      <span style=color:#f92672>labels</span>:
        <span style=color:#f92672>tier</span>: <span style=color:#ae81ff>node</span>
        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>flannel</span>
    <span style=color:#f92672>spec</span>:
      <span style=color:#f92672>affinity</span>:
        <span style=color:#f92672>nodeAffinity</span>:
          <span style=color:#f92672>requiredDuringSchedulingIgnoredDuringExecution</span>:
            <span style=color:#f92672>nodeSelectorTerms</span>:
              - <span style=color:#f92672>matchExpressions</span>:
                  - <span style=color:#f92672>key</span>: <span style=color:#ae81ff>kubernetes.io/os</span>
                    <span style=color:#f92672>operator</span>: <span style=color:#ae81ff>In</span>
                    <span style=color:#f92672>values</span>:
                      - <span style=color:#ae81ff>linux</span>
      <span style=color:#f92672>hostNetwork</span>: <span style=color:#66d9ef>true</span>
      <span style=color:#f92672>priorityClassName</span>: <span style=color:#ae81ff>system-node-critical</span>
      <span style=color:#f92672>tolerations</span>:
        - <span style=color:#f92672>operator</span>: <span style=color:#ae81ff>Exists</span>
          <span style=color:#f92672>effect</span>: <span style=color:#ae81ff>NoSchedule</span>
      <span style=color:#f92672>serviceAccountName</span>: <span style=color:#ae81ff>flannel</span>
      <span style=color:#f92672>initContainers</span>:
        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>install-cni</span>
          <span style=color:#f92672>image</span>: <span style=color:#ae81ff>quay.io/coreos/flannel:v0.13.1-rc2</span>
          <span style=color:#f92672>command</span>:
            - <span style=color:#ae81ff>cp</span>
          <span style=color:#f92672>args</span>:
            - -<span style=color:#ae81ff>f</span>
            - <span style=color:#ae81ff>/etc/kube-flannel/cni-conf.json</span>
            - <span style=color:#ae81ff>/etc/cni/net.d/10-flannel.conflist</span>
          <span style=color:#f92672>volumeMounts</span>:
            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cni</span>
              <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/etc/cni/net.d</span>
            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel-cfg</span>
              <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/etc/kube-flannel/</span>
      <span style=color:#f92672>containers</span>:
        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-flannel</span>
          <span style=color:#f92672>image</span>: <span style=color:#ae81ff>quay.io/coreos/flannel:v0.13.1-rc2</span>
          <span style=color:#f92672>command</span>:
            - <span style=color:#ae81ff>/opt/bin/flanneld</span>
          <span style=color:#f92672>args</span>:
            - --<span style=color:#ae81ff>ip-masq</span>
            - --<span style=color:#ae81ff>kube-subnet-mgr</span>
          <span style=color:#f92672>resources</span>:
            <span style=color:#f92672>requests</span>:
              <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;100m&#34;</span>
              <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;50Mi&#34;</span>
            <span style=color:#f92672>limits</span>:
              <span style=color:#f92672>cpu</span>: <span style=color:#e6db74>&#34;100m&#34;</span>
              <span style=color:#f92672>memory</span>: <span style=color:#e6db74>&#34;50Mi&#34;</span>
          <span style=color:#f92672>securityContext</span>:
            <span style=color:#f92672>privileged</span>: <span style=color:#66d9ef>false</span>
            <span style=color:#f92672>capabilities</span>:
              <span style=color:#f92672>add</span>: [<span style=color:#e6db74>&#34;NET_ADMIN&#34;</span>, <span style=color:#e6db74>&#34;NET_RAW&#34;</span>]
          <span style=color:#f92672>env</span>:
            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>POD_NAME</span>
              <span style=color:#f92672>valueFrom</span>:
                <span style=color:#f92672>fieldRef</span>:
                  <span style=color:#f92672>fieldPath</span>: <span style=color:#ae81ff>metadata.name</span>
            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>POD_NAMESPACE</span>
              <span style=color:#f92672>valueFrom</span>:
                <span style=color:#f92672>fieldRef</span>:
                  <span style=color:#f92672>fieldPath</span>: <span style=color:#ae81ff>metadata.namespace</span>
          <span style=color:#f92672>volumeMounts</span>:
            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>run</span>
              <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/run/flannel</span>
            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel-cfg</span>
              <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/etc/kube-flannel/</span>
      <span style=color:#f92672>volumes</span>:
        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>run</span>
          <span style=color:#f92672>hostPath</span>:
            <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/run/flannel</span>
        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cni</span>
          <span style=color:#f92672>hostPath</span>:
            <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/etc/cni/net.d</span>
        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>flannel-cfg</span>
          <span style=color:#f92672>configMap</span>:
            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>kube-flannel-cfg</span>
</code></pre></div><h3 id=kubeadm_joinsh>
kubeadm_join.sh
<a class=anchor href=#kubeadm_joinsh>#</a>
</h3>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#75715e>#!/bin/bash
</span><span style=color:#75715e></span>
sudo kubeadm join ip:6443 --token wxjhuh.z9ru6bbz990m7v0i --discovery-token-ca-cert-hash sha256:8809f1cc27401d704faa104008a48034b51e8e5ef8f4a8f33f0e267db0124a2f
</code></pre></div><h3 id=一些问题>
一些问题
<a class=anchor href=#%e4%b8%80%e4%ba%9b%e9%97%ae%e9%a2%98>#</a>
</h3>
<p>1.k8s: reset 后出现 coredns 一直处于 ContainerCreating 状态</p>
<p>
<a href=https://bbs.huaweicloud.com/forum/thread-60243-1-1.html>解决方法 1</a></p>
<p>2.k8s: connet to localhost:8080 failed</p>
<p>kubeadm init 成功后，执行<code>sudo kubectl apply -f kube-flannel.yml</code>，出现错误：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre></div><p>
<a href=https://github.com/kubernetes/kubernetes/issues/44665>issue</a></p>
<p>原来是因为没有执行<code>export KUBECONFIG=/etc/kubernetes/admin.conf</code>命令的时候使用 kubectl 是不能在前面加 sudo 的。</p>
<p>所以，直接执行<code>kubectl apply -f kube-flannel.yml</code>就正常了。</p>
<h4 id=基于-wsl2-搭建>
基于 wsl2 搭建
<a class=anchor href=#%e5%9f%ba%e4%ba%8e-wsl2-%e6%90%ad%e5%bb%ba>#</a>
</h4>
<p>
<a href=https://www.qikqiak.com/post/deploy-k8s-on-win-use-wsl2/>基于 wsl2 搭建</a></p>
<h3 id=deploy>
Deploy
<a class=anchor href=#deploy>#</a>
</h3>
<p>k8s: 部署应用之 deployment service pod</p>
<h4 id=问题-1>
问题 1
<a class=anchor href=#%e9%97%ae%e9%a2%98-1>#</a>
</h4>
<p>在创建 deployment 时，如果想从本地获取镜像，需要将 yaml 配置文件里的<code>imagePullPolicy</code>，镜像拉取机制，从<code>Always</code>改为<code>IfNotPresent</code>。</p>
<h4 id=问题-2>
问题 2
<a class=anchor href=#%e9%97%ae%e9%a2%98-2>#</a>
</h4>
<p>怎么让 pod 里的应用访问到非集群内部的本机数据库实例呢？</p>
<p>通过新建一个 service，指定为<code>type:ExternalName</code>或<code>Endpoints</code>，并把想要访问的数据库实例信息写到配置上。</p>
<p>
<a href=https://juejin.cn/post/6844903693918306312>参照 1</a></p>
<p>
<a href=https://www.kubernetes.org.cn/4317.html>参照 2</a></p>
<p>
<a href=https://www.cnblogs.com/ericnie/p/7560280.html>参照 3</a></p>
<p>
<a href=https://juejin.cn/post/6844903693918306312>参照 4</a></p>
<p>
<a href=https://www.cnblogs.com/kuku0223/p/10898068.html>参照 5</a></p>
<p>解决：在本机的数据库貌似不能访问到，只好在另外的虚拟机上安装 db，然后配置该虚拟机的 ip 地址到 endpoint。</p>
<h4 id=使用-service-将应用暴露到公网>
使用 service 将应用暴露到公网
<a class=anchor href=#%e4%bd%bf%e7%94%a8-service-%e5%b0%86%e5%ba%94%e7%94%a8%e6%9a%b4%e9%9c%b2%e5%88%b0%e5%85%ac%e7%bd%91>#</a>
</h4>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=color:#75715e># 针对deployment hello-world以NodePort方式、example-service名称、端口是port-value暴露应用到公网</span>
kubectl expose deployment hello-world --type<span style=color:#f92672>=</span>NodePort --name<span style=color:#f92672>=</span>example-service -- port<span style=color:#f92672>=</span>port-value
</code></pre></div><p>通过公网访问时，需要先使用<code>kubectl describe svc</code>拿到 example-service 服务的 NodePort 值，再结合机器自身的 ip 地址，组成 ip:NodePort 值访问，如：
<a href=http://192.168.9.16:32256/>http://192.168.9.16:32256/</a>。</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>jd@wnode1:~/Project/jdnote$ kubectl describe svc
Name:                     jdnote-server-service
Namespace:                default
Labels:                   app<span style=color:#f92672>=</span>jdnote-server
Annotations:              &lt;none&gt;
Selector:                 app<span style=color:#f92672>=</span>jdnote-server
Type:                     NodePort
IP Families:              &lt;none&gt;
IP:                       192.168.1.198
IPs:                      192.168.1.198
Port:                     &lt;unset&gt;  8890/TCP
TargetPort:               8890/TCP
NodePort:                 &lt;unset&gt;  32256/TCP <span style=color:#75715e># 这个值作为端口</span>
Endpoints:                10.46.129.11:8890
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>jd@wnode1:~/Project/jdnote$ ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>65536</span> qdisc noqueue state UNKNOWN group default qlen <span style=color:#ae81ff>1000</span>
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#ae81ff>1500</span> qdisc fq_codel state UP group default qlen <span style=color:#ae81ff>1000</span>
    link/ether 08:00:27:cf:aa:13 brd ff:ff:ff:ff:ff:ff
    inet 192.168.9.16/24 brd 192.168.9.255 scope global dynamic enp0s3 <span style=color:#75715e># 192.168.9.16作为ip</span>
       valid_lft 9843sec preferred_lft 9843sec
    inet6 fe80::a00:27ff:fecf:aa13/64 scope link
       valid_lft forever preferred_lft forever
</code></pre></div><p>
<a href=https://kubernetes.io/zh/docs/tasks/access-application-cluster/service-access-application-cluster/>参照</a></p>
<p>
<a href=https://dominik-tornow.medium.com/kubernetes-networking-22ea81af44d0>网络介绍</a></p>
<h4 id=k8s-ingress>
k8s: ingress
<a class=anchor href=#k8s-ingress>#</a>
</h4>
<p>
<a href=https://kubernetes.io/zh/docs/concepts/services-networking/ingress/>k8s: ingress</a></p>
<p>让外网能访问到 k8s 里的应用。</p>
<p>单个服务：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Ingress</span> <span style=color:#75715e># kind必须指定为ingress</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>minimal-ingress</span> <span style=color:#75715e># 名称</span>
  <span style=color:#f92672>annotations</span>: <span style=color:#75715e># 注解</span>
    <span style=color:#f92672>nginx.ingress.kubernetes.io/rewrite-target</span>: <span style=color:#ae81ff>/</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>rules</span>: <span style=color:#75715e># 规则</span>
    - <span style=color:#f92672>http</span>:
        <span style=color:#f92672>paths</span>: <span style=color:#75715e># 路径</span>
          - <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/testpath</span>
            <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
            <span style=color:#f92672>backend</span>: <span style=color:#75715e># 重定向到service</span>
              <span style=color:#f92672>service</span>:
                <span style=color:#f92672>name</span>: <span style=color:#ae81ff>test</span> <span style=color:#75715e># 必须存在test service</span>
                <span style=color:#f92672>port</span>:
                  <span style=color:#f92672>number</span>: <span style=color:#ae81ff>80</span> <span style=color:#75715e># service的端口</span>
</code></pre></div><p>多个服务：</p>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
<span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Ingress</span>
<span style=color:#f92672>metadata</span>:
  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>ingress-wildcard-host</span>
<span style=color:#f92672>spec</span>:
  <span style=color:#f92672>rules</span>:
    - <span style=color:#f92672>host</span>: <span style=color:#e6db74>&#34;foo.bar.com&#34;</span> <span style=color:#75715e># 域名，表示该域名的流量将被转到配置的service</span>
      <span style=color:#f92672>http</span>:
        <span style=color:#f92672>paths</span>:
          - <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
            <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#34;/bar&#34;</span>
            <span style=color:#f92672>backend</span>:
              <span style=color:#f92672>service</span>:
                <span style=color:#f92672>name</span>: <span style=color:#ae81ff>service1</span>
                <span style=color:#f92672>port</span>:
                  <span style=color:#f92672>number</span>: <span style=color:#ae81ff>80</span>
    - <span style=color:#f92672>host</span>: <span style=color:#e6db74>&#34;*.foo.com&#34;</span>
      <span style=color:#f92672>http</span>:
        <span style=color:#f92672>paths</span>:
          - <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
            <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#34;/foo&#34;</span>
            <span style=color:#f92672>backend</span>:
              <span style=color:#f92672>service</span>:
                <span style=color:#f92672>name</span>: <span style=color:#ae81ff>service2</span>
                <span style=color:#f92672>port</span>:
                  <span style=color:#f92672>number</span>: <span style=color:#ae81ff>80</span>
</code></pre></div><p>查看：<code>kubectl describe ingress minimal-ingress</code></p>
<p>
<a href=https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers/>可选 ingress 控制器</a></p>
<p>Ingress 只是 Kubernetes 中的一种配置信息；Ingress Controller 才是监听 80/443 端口，并根据 Ingress 上配置的路由信息执行 HTTP 路由转发的组件。</p>
<p>
<a href=https://doc.traefik.io/traefik/providers/kubernetes-ingress/>traefik 提供的 k8s 控制器</a></p>
<p>
<a href=https://jimmysong.io/kubernetes-handbook/practice/traefik-ingress-installation.html>使用 traefik</a></p>
<h2 id=list-watch-模式>
list-watch 模式
<a class=anchor href=#list-watch-%e6%a8%a1%e5%bc%8f>#</a>
</h2>
<p>
<a href=https://zhuanlan.zhihu.com/p/59660536>来源</a></p>
<blockquote>
<p>谈谈 List-Watch 的设计理念</p>
<p>当设计优秀的一个异步消息的系统时，对消息机制有至少如下四点要求：</p>
<blockquote>
<p>消息可靠性</p>
<p>消息实时性</p>
<p>消息顺序性</p>
<p>高性能</p>
<p>首先消息必须是可靠的，list 和 watch 一起保证了消息的可靠性，避免因消息丢失而造成状态不一致场景。</p>
<p>具体而言，list API 可以查询当前的资源及其对应的状态(即期望的状态)，客户端通过拿期望的状态和实际的状态进行对比，纠正状态不一致的资源。Watch API 和 apiserver 保持一个长链接，接收资源的状态变更事件并做相应处理。如果仅调用 watch API，若某个时间点连接中断，就有可能导致消息丢失，所以需要通过 list API 解决消息丢失的问题。从另一个角度出发，我们可以认为 list API 获取全量数据，watch API 获取增量数据。虽然仅仅通过轮询 list API，也能达到同步资源状态的效果，但是存在开销大，实时性不足的问题。</p>
</blockquote>
<p>消息必须是实时的，list-watch 机制下，每当 apiserver 的资源产生状态变更事件，都会将事件及时的推送给客户端，从而保证了消息的实时性。</p>
<p>消息的顺序性也是非常重要的，在并发的场景下，客户端在短时间内可能会收到同一个资源的多个事件，对于关注最终一致性的 K8S 来说，它需要知道哪个是最近发生的事件，并保证资源的最终状态如同最近事件所表述的状态一样。K8S 在每个资源的事件中都带一个 resourceVersion 的标签，这个标签是递增的数字，所以当客户端并发处理同一个资源的事件时，它就可以对比 resourceVersion 来保证最终的状态和最新的事件所期望的状态保持一致。</p>
<p>List-watch 还具有高性能的特点，虽然仅通过周期性调用 list API 也能达到资源最终一致性的效果，但是周期性频繁的轮询大大的增大了开销，增加 apiserver 的压力。而 watch 作为异步消息通知机制，复用一条长链接，保证实时性的同时也保证了性能。</p>
</blockquote>
<h2 id=调试>
调试
<a class=anchor href=#%e8%b0%83%e8%af%95>#</a>
</h2>
<p>golang: dlv 调试 k8s 容器里的 go 进程</p>
<h3 id=使用容器-exec-进行调试>
使用容器 exec 进行调试
<a class=anchor href=#%e4%bd%bf%e7%94%a8%e5%ae%b9%e5%99%a8-exec-%e8%bf%9b%e8%a1%8c%e8%b0%83%e8%af%95>#</a>
</h3>
<p>如果 容器镜像 包含调试程序(dlv, gdb)， 比如从 Linux 和 Windows 操作系统基础镜像构建的镜像，你可以使用 kubectl exec 命令 在特定的容器中运行一些命令：</p>
<p><code>kubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- ${CMD} ${ARG1} ${ARG2} ... ${ARGN}</code></p>
<blockquote>
<p>说明： -c ${CONTAINER_NAME} 是可选择的。如果 Pod 中仅包含一个容器，就可以忽略它。</p>
</blockquote>
<p>例如，要查看正在运行的 Cassandra pod 中的日志，可以运行：</p>
<p><code>kubectl exec cassandra -- cat /var/log/cassandra/system.log</code></p>
<p>你可以在 kubectl exec 命令后面加上 -i 和 -t 来运行一个连接到你的终端的 Shell，比如：</p>
<p><code>kubectl exec -it cassandra -- sh</code></p>
<p>
<a href=https://kubernetes.io/zh/docs/tasks/debug-application-cluster/debug-running-pod/>参照 1</a></p>
<p>
<a href=https://kubernetes.io/zh/docs/tasks/debug-application-cluster/get-shell-running-container/>参照 2</a></p>
<h3 id=尝试>
尝试
<a class=anchor href=#%e5%b0%9d%e8%af%95>#</a>
</h3>
<div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh>$ kubectl exec -it skm-7fb6bd989c-kg9xj -n skm-system -- sh <span style=color:#75715e># 使用 kubectl 进入容器的 sh 界面</span>

sh-4.4# dlv exec /root/Project/rancher/bin/rancher <span style=color:#75715e># exec 报错：版本太旧</span>
Version of Delve is too old <span style=color:#66d9ef>for</span> this version of Go <span style=color:#f92672>(</span>maximum supported version 1.13, suppress this error with --check-go-version<span style=color:#f92672>=</span>false<span style=color:#f92672>)</span>
sh-4.4# dlv attach /root/Project/rancher/bin/rancher <span style=color:#75715e># attach 需要指定 pid</span>
Invalid pid: /root/Project/rancher/bin/rancher
sh-4.4# dlv attach <span style=color:#ae81ff>653</span>                              <span style=color:#75715e># 通过 ps aux 拿到 pid</span>
Type <span style=color:#e6db74>&#39;help&#39;</span> <span style=color:#66d9ef>for</span> list of commands.
<span style=color:#f92672>(</span>dlv<span style=color:#f92672>)</span>

</code></pre></div><p>
<a href=https://chai2010.cn/advanced-go-programming-book/ch3-asm/ch3-09-debug.html>dlv 使用</a></p>
</article>
<footer class=book-footer>
<div class="flex flex-wrap justify-between">
</div>
<script>(function(){function a(c){const a=window.getSelection(),b=document.createRange();b.selectNodeContents(c),a.removeAllRanges(),a.addRange(b)}document.querySelectorAll("pre code").forEach(b=>{b.addEventListener("click",function(c){a(b.parentElement),navigator.clipboard&&navigator.clipboard.writeText(b.parentElement.textContent)})})})()</script>
</footer>
<div class=book-comments>
<div id=disqus_thread></div>
<script type=application/javascript>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById('disqus_thread').innerHTML='Disqus comments not available by default when the website is previewed locally.';return}var b=document,a=b.createElement('script');a.async=!0,a.src='//donno272.disqus.com/embed.js',a.setAttribute('data-timestamp',+new Date),(b.head||b.body).appendChild(a)})()</script>
<noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript>
<a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a>
</div>
<label for=menu-control class="hidden book-menu-overlay"></label>
</div>
<aside class=book-toc>
<div class=book-toc-content>
<nav id=TableOfContents>
<ul>
<li>
<ul>
<li><a href=#what>What</a></li>
<li><a href=#why>Why</a></li>
<li><a href=#how>How</a>
<ul>
<li><a href=#install>Install</a></li>
<li><a href=#k8s-初始化集群>k8s: 初始化集群</a></li>
</ul>
</li>
<li><a href=#kube-flannelyml>kube-flannel.yml</a>
<ul>
<li><a href=#kubeadm_joinsh>kubeadm_join.sh</a></li>
<li><a href=#一些问题>一些问题</a></li>
<li><a href=#deploy>Deploy</a></li>
</ul>
</li>
<li><a href=#list-watch-模式>list-watch 模式</a></li>
<li><a href=#调试>调试</a>
<ul>
<li><a href=#使用容器-exec-进行调试>使用容器 exec 进行调试</a></li>
<li><a href=#尝试>尝试</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</div>
</aside>
</main>
</body>
</html>