[{"id":0,"href":"/posts/2021/12/k8s/","title":"k8s","section":"12","content":"What #  docker 带来容器之风，以致容器多不胜数。如何编排和管理众多容器，使得它们同心协力办好事情，即成为了当下最大的课题。\n为此，k8s 应运而生。\n容器，通讯，存储，配置。\nWhy #  为编排和管理数量众多的容器。\nHow #  Install #  k8s: 集群搭建所需资源 #   One or more machines running one of:\n Ubuntu 16.04+\nDebian 9+\nCentOS 7+\nRed Hat Enterprise Linux (RHEL) 7+\nFedora 25+\nHypriotOS v1.0.1+\nFlatcar Container Linux (tested with 2512.3.0)\n 2 GB or more of RAM per machine (any less will leave little room for your apps).\n2 CPUs or more.\nFull network connectivity between all machines in the cluster (public or private network is fine).\nUnique hostname, MAC address, and product_uuid for every node. See here for more details.\nCertain ports are open on your machines. See here for more details.\nSwap disabled. You MUST disable swap in order for the kubelet to work properly.\n  参考官方文档\n本地机器起多个虚拟机搭建 #  使用 virtualBox 创建三台虚拟机 #  1.用 NAT 和 host only 网络模式\n virtualBox 安装比较简单，不再介绍，GUI 工具用起来也很方便，这部分只介绍我认为需要提示的部分。\n内存推荐 2048M, CPU 推荐 2 个\n默认只有一个 NAT 适配器，添加一个 Host-Only Adapter。NAT 适配器是虚拟机用来访问互联网的，Host-Only 适配器是用来虚拟机之间通信的。\n以 Normal Start 方式启动虚拟机安装完系统以后，因为是 server 版镜像，所以没有图形界面，直接使用用户名密码登录即可。\n修改配置，enp0s8 使用静态 IP。配置请参考 SSH between Mac OS X host and Virtual Box guest。注意配置时将其中的网络接口名改成你自己的 Host-Only Adapter 对应的接口。\n一台虚拟机创建完成以后可以使用 clone 方法复制出两台节点出来，注意 clone 时为新机器的网卡重新初始化 MAC 地址。\n三台虚拟机的静态 IP 都配置好以后就可以使用 ssh 在本地主机的终端上操作三台虚机了。虚机使用 Headless Start 模式启动\n  参照\n 参照 2\n2.用桥接网络模式\nvm: 虚拟机上安装 k8s\n先添加 key：https_proxy=http://192.168.56.1:51837 curl -s -v https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n然后添加 source：\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF 最后更新：sudo apt -o Acquire::http::proxy=\u0026quot;http://192.168.56.1:51837\u0026quot; update\nkubeadm\n安装：sudo apt -o Acquire::https::proxy=\u0026quot;http://192.168.56.1:51837\u0026quot; install -y kubeadm\nkubelet\n安装：sudo apt -o Acquire::https::proxy=\u0026quot;http://192.168.56.1:51837\u0026quot; install -y kubelet\nkubectl\n安装：sudo apt -o Acquire::https::proxy=\u0026quot;http://192.168.56.1:51837\u0026quot; install -y kubectl\ndocker\n 参照\n从上面的参照可以看到除了 docker 之外，还可以选择其它运行时。\nk8s: 三个工具\nkubeadm\n作用：用来初始化集群的指令。\n使用：\n 在一台机器上执行kubeadm init，初始化集群，该机器作为集群 master；初始化成功后会返回\u0026lt;arguments-returned-from-init\u0026gt;，作为其它机器加入该集群的参数 。\n在另一台机器上执行kubeadm join \u0026lt;arguments-returned-from-init\u0026gt;。\n如果想添加更多机器，请重复kubeadm join指令。\n kubelet\n作用：在集群中的每个节点上用来启动 Pod 和容器等。\n在每个节点上运行的节点代理。它可以向 apiserver 注册节点。\n The kubelet works in terms of a PodSpec. A PodSpec is a YAML or JSON object that describes a pod.\n\u0026ndash; kubelet 在一系列 pod 规范里工作。一个 pod 规范是一个描述 pod 的 yaml 或 json 对象。\n kubectl\n作用：用来与集群通信的命令行工具。\n创建资源，暴露服务。\n 参照\nk8s: 初始化集群 #  关闭 swap 交换空间 #  执行命令：sudo swapoff -a，并将文件/etc/fstab里关于 swap 的行注释掉，然后重启机器\n为什么要关闭呢？\n issue 上的讨论 1\n issue 上的讨论 2\n having swap available has very strange and bad interactions with memory limits\nkubernetes 的想法是将实例紧密包装到尽可能接近 100％。 所有的部署应该与 CPU /内存限制固定在一起。 所以如果调度程序发送一个 pod 到一台机器，它不应该使用交换。 设计者不想交换，因为它会减慢速度。\n所以关闭 swap 主要是为了性能考虑。\n当然为了一些节省资源的场景，比如运行容器数量较多，可添加 kubelet 参数 \u0026ndash;fail-swap-on=false 来解决。\n 初始化 #  先通过命令sudo kubeadm config print init-defaults \u0026gt; init-default.yaml生成默认配置文件。\n 在生成的 yaml 配置文件里修改:\nadvertiseAddress: 192.168.9.43，其中的 ip 地址为ip a拿到的地址。\nnetworking: dnsDomain: cluster.local podSubnet: 10.46.128.0/21 # 这个 ip 将要用在安装成功后的 pod network 配置里。 serviceSubnet: 192.168.1.0/24\n 然后在初始化命令使用该配置文件：sudo kubeadm init --config=init-default.yaml --v=5\n出现警告：\n [preflight] Running pre-flight checks\n[WARNING IsDockerSystemdCheck]: detected \u0026ldquo;cgroupfs\u0026rdquo; as the Docker cgroup driver. The recommended driver is \u0026ldquo;systemd\u0026rdquo;. Please follow the guide at https://kubernetes.io/docs/setup/cri/\n 里面说到 cgroup 驱动用了 cgroupfs，而不是 systemd。可参照 官方文档修改设置。\n然后会到k8s.gcr.io获取镜像，这时又出现超时错误。应该是墙导致的，需要使用代理或 改用镜像站。\n 镜像站也关掉了，怎么办？\n使用 docker 拉取镜像，然后修改 tag：\n 先看所需镜像：sudo kubeadm config images list\n k8s.gcr.io/kube-apiserver:v1.20.3\nk8s.gcr.io/kube-controller-manager:v1.20.3\nk8s.gcr.io/kube-scheduler:v1.20.3\nk8s.gcr.io/kube-proxy:v1.20.3\nk8s.gcr.io/pause:3.2\nk8s.gcr.io/etcd:3.4.13-0\nk8s.gcr.io/coredns:1.7.0\n 逐个从 docker hub 上搜到并获取镜像：sudo docker pull aiotceo/kube-apiserver:v1.20.3\n sudo docker pull aiotceo/kube-apiserver:v1.20.3\nsudo docker pull aiotceo/kube-controller-manager:v1.20.3\nsudo docker pull aiotceo/kube-scheduler:v1.20.3\nsudo docker pull aiotceo/kube-proxy:v1.20.3\nsudo docker pull aiotceo/pause:3.2\nsudo docker pull bitnami/etcd:3.4.13\nsudo docker pull aiotceo/coredns:1.7.0\n 修改 tag：sudo docker tag aiotceo/kube-apiserver:v1.20.3 k8s.gcr.io/kube-apiserver:v1.20.3\n最后删除：sudo docker rmi aiotceo/kube-apiserver:v1.20.3\n[preflight] You can also perform this action in beforehand using \u0026lsquo;kubeadm config images pull\u0026rsquo;\n\u0026ndash; 拉取镜像也可以提前使用kubeadm config images pull完成。\n kubelet 未启动错误：\n [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026ldquo;/etc/kubernetes/manifests\u0026rdquo;. This can take up to 4m0s I0218 09:29:21.630514 98836 request.go:943] Got a Retry-After 1s response for attempt 1 to https://10.0.2.15:6443/healthz?timeout=10s \u0026gt; [kubelet-check] Initial timeout of 40s passed. I0218 09:29:43.755024 98836 request.go:943] Got a Retry-After 1s response for attempt 1 to https://10.0.2.15:6443/healthz?timeout=10s I0218 09:30:21.758311 98836 request.go:943] Got a Retry-After 1s response for attempt 1 to https://10.0.2.15:6443/healthz?timeout=10s I0218 09:32:24.612682 98836 request.go:943] Got a Retry-After 1s response for attempt 1 to https://10.0.2.15:6443/healthz?timeout=10s\n Unfortunately, an error has occurred: timed out waiting for the condition This error is likely caused by: - The kubelet is not running - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled) If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands: - \u0026#39;systemctl status kubelet\u0026#39; - \u0026#39;journalctl -xeu kubelet\u0026#39; Additionally, a control plane component may have crashed or exited when started by the container runtime. To troubleshoot, list all containers using your preferred container runtimes CLI. Here is one example how you may list all Kubernetes containers running in docker: - \u0026#39;docker ps -a | grep kube | grep -v pause\u0026#39; Once you have found the failing container, you can inspect its logs with: - \u0026#39;docker logs CONTAINERID\u0026#39;  原来是在关闭了 swap 后要重启\n初始化过程中报错了之后需要重置一下才行：sudo kubeadm reset --v=5\n否则会报错：\n [preflight] Some fatal errors occurred:\n [ERROR Port-10259]: Port 10259 is in use [ERROR Port-10257]: Port 10257 is in use [ERROR FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml]: /etc/kubernetes/manifests/kube-apiserver.yaml already exists [ERROR FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml]: /etc/kubernetes/manifests/kube-controller-manager.yaml already exists [ERROR FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml]: /etc/kubernetes/manifests/kube-scheduler.yaml already exists [ERROR FileAvailable--etc-kubernetes-manifests-etcd.yaml]: /etc/kubernetes/manifests/etcd.yaml already exists [ERROR Port-10250]: Port 10250 is in use [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` error execution phase preflight 查看kubelet日志：journalctl -xeu kubelet\nFailed to connect to 10.0.2.15 port 6443: Connection refused:\n 除了防火墙和 swap，还要关闭 selinux\n 查看防火墙状态：systemctl status firewalld\n临时关闭 selinux：\n sudo setenforce 0\n 永久关闭：\n 执行sestatus查看 selinux 状态，需要先安装工具sudo apt install policycoreutils\n查看后，再去编辑配置文件sudo vim /etc/selinux/config，改为SELINUX=disabled\n   需要翻墙的情况下\n修改 docker 的 cgroup driver 为 systemd #  为了确保 docker 和 kubelet 的 cgroup driver 一样，需要将 docker 的 cgroup driver 改为 systemd。\n查看：sudo docker info，发现Cgroup Driver: cgroupfs\n修改：sudo vim /etc/docker/daemon.json，添加以下内容：\n{ \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } 重启：sudo systemctl daemon-reload sudo systemctl restart docker\n再次查看 docker info，会看到Cgroup Driver: systemd\nkubectl #  执行sudo kubelet时出现错误：\n failed to get the kubelet\u0026rsquo;s cgroup: cpu and memory cgroup hierarchy not unified. cpu: /user.slice, memory: /user.slice/user-1000.slice/session-1.scope. Kubelet system container metrics may be missing.\n 难道是不能这样直接执行，需要用 systemctl 来执行：sudo systemctl start kubelet.service\n通过查看sudo cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf配置文件，发现里面有很多 env 设置。\n在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf配置文件里添加--cgroup-driver=systemd配置。\n是不是虚拟机创建时选择的网卡错了（用了 NAT 和 host only），导致上面的这么多问题呢？\n 换成 NAT network 或者桥接试下。\n With bridged networking, Oracle VM VirtualBox uses a device driver on your host system that filters data from your physical network adapter. This driver is therefore called a net filter driver. This enables Oracle VM VirtualBox to intercept data from the physical network and inject data into it, effectively creating a new network interface in software. When a guest is using such a new software interface, it looks to the host system as though the guest were physically connected to the interface using a network cable. The host can send data to the guest through that interface and receive data from it. This means that you can set up routing or bridging between the guest and the rest of your network.\n  vbox 官方文档关于网卡的介绍\n成功 #  成功后会有以下信息：\n其中需要注意部分信息是需要执行的命令，还有其它节点添加到集群时需要用到的信息。\n[addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.9.43:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:5660805073db31916952821a8751ca0ee0644ce4205f616805f8a7f175ff8b33  添加 pod network：\n 下载配置文件：wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n编辑配置：修改\nnet-conf.json: |\n{ \u0026ldquo;Network\u0026rdquo;: \u0026ldquo;10.46.128.0/21\u0026rdquo;, # 这个地址要与 kubeadm 指定的配置文件里的 podSubnet 里的 ip 一致 \u0026ldquo;Backend\u0026rdquo;: { \u0026ldquo;Type\u0026rdquo;: \u0026ldquo;vxlan\u0026rdquo; } }\n最后执行命令: kubectl apply -f kube-flannel.yml\n 最后，查看 pods 状态：kubectl get pods --all-namespaces\njd@jnode:~$ kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-74ff55c5b-f76bb 1/1 Running 0 5m52s kube-system coredns-74ff55c5b-frgkw 1/1 Running 0 5m52s kube-system etcd-jnode 1/1 Running 0 6m8s kube-system kube-apiserver-jnode 1/1 Running 0 6m8s kube-system kube-controller-manager-jnode 1/1 Running 0 6m8s kube-system kube-flannel-ds-xghcx 1/1 Running 0 4m4s kube-system kube-proxy-nnbbx 1/1 Running 0 5m52s kube-system kube-scheduler-jnode 1/1 Running 0 6m8s 全部正常运行，完成。\n错误 #  k8s 的安装过程，可谓是一波三折。下面整理一下遇到的错误：\n Error registering network: failed to acquire lease: node \u0026ldquo;nodeName\u0026rdquo; pod cidr not assigned \n 确保 Network 的值与 podSubnet 一致即可。\n 添加 worker 节点 #  在初始化完成后的输出信息里有：\nThen you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.9.43:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:5660805073db31916952821a8751ca0ee0644ce4205f616805f8a7f175ff8b33 这个命令就是 worker 要加入集群时需要执行的命令。\n在执行上述命令之前，需要 重新生成 token 和 hash:\n# 重新生成新的token kubeadm token create kubeadm token list # 获取ca证书sha256编码hash值，拿到的hash值要记录下来 openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39; # 节点加入集群，token和hash值使用上面生成的 kubeadm join ip:6443 --token xxx --discovery-token-ca-cert-hash sha256:xxx 最后在主节点，执行：kubectl get nodes查看已加入的节点。\n重启之后要等一会才会正常：\n$ kubectl get nodes The connection to the server 192.168.9.43:6443 was refused - did you specify the right host or port? 如果没有执行export KUBECONFIG=/etc/kubernetes/admin.conf，那么在使用 kubectl 命令时就不能在前面加 sudo 了。\n在主节点成功看到 worker node 之后，还需要将主节点里的~/.kube/config文件复制到 worker node 上：\n# 在主节点上使用scp将配置文件复制到worker scp ~/.kube/config xx@xxx.xxx.xxx.xxx:~/.kube/config 否则会报错：\nkubectl describe node The connection to the server localhost:8080 was refused - did you specify the right host or port? 复制配置到 worker 后，在 worker 节点上执行kubectl describe node，就能看到节点信息。\n 参照\nk8s: 初始化配置文件 #  init-default.yaml #  apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.9.43 # 本机的ip地址，可通过`ip a`获取 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: jnode taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: k8s.gcr.io kind: ClusterConfiguration kubernetesVersion: v1.20.0 networking: dnsDomain: cluster.local podSubnet: 10.46.128.0/21 # pod subnet值需要与kube-flannel.yml里的网络设置一致 serviceSubnet: 192.168.1.0/24 # 跟本机ip同一网段 scheduler: {} kube-flannel.yml #  --- apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: psp.flannel.unprivileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default apparmor.security.beta.kubernetes.io/defaultProfileName: runtime/default spec: privileged: false volumes: - configMap - secret - emptyDir - hostPath allowedHostPaths: - pathPrefix: \u0026#34;/etc/cni/net.d\u0026#34; - pathPrefix: \u0026#34;/etc/kube-flannel\u0026#34; - pathPrefix: \u0026#34;/run/flannel\u0026#34; readOnlyRootFilesystem: false # Users and groups runAsUser: rule: RunAsAny supplementalGroups: rule: RunAsAny fsGroup: rule: RunAsAny # Privilege Escalation allowPrivilegeEscalation: false defaultAllowPrivilegeEscalation: false # Capabilities allowedCapabilities: [\u0026#34;NET_ADMIN\u0026#34;, \u0026#34;NET_RAW\u0026#34;] defaultAddCapabilities: [] requiredDropCapabilities: [] # Host namespaces hostPID: false hostIPC: false hostNetwork: true hostPorts: - min: 0 max: 65535 # SELinux seLinux: # SELinux is unused in CaaSP rule: \u0026#34;RunAsAny\u0026#34; --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel rules: - apiGroups: [\u0026#34;extensions\u0026#34;] resources: [\u0026#34;podsecuritypolicies\u0026#34;] verbs: [\u0026#34;use\u0026#34;] resourceNames: [\u0026#34;psp.flannel.unprivileged\u0026#34;] - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - get - apiGroups: - \u0026#34;\u0026#34; resources: - nodes verbs: - list - watch - apiGroups: - \u0026#34;\u0026#34; resources: - nodes/status verbs: - patch --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: flannel roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: flannel subjects: - kind: ServiceAccount name: flannel namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: flannel namespace: kube-system --- kind: ConfigMap apiVersion: v1 metadata: name: kube-flannel-cfg namespace: kube-system labels: tier: node app: flannel data: cni-conf.json: |{ \u0026#34;name\u0026#34;: \u0026#34;cbr0\u0026#34;, \u0026#34;cniVersion\u0026#34;: \u0026#34;0.3.1\u0026#34;, \u0026#34;plugins\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;flannel\u0026#34;, \u0026#34;delegate\u0026#34;: { \u0026#34;hairpinMode\u0026#34;: true, \u0026#34;isDefaultGateway\u0026#34;: true } }, { \u0026#34;type\u0026#34;: \u0026#34;portmap\u0026#34;, \u0026#34;capabilities\u0026#34;: { \u0026#34;portMappings\u0026#34;: true } } ] } net-conf.json: |{ \u0026#34;Network\u0026#34;: \u0026#34;10.46.128.0/21\u0026#34;, # 与init-default.yaml配置文件里的podSubnet值一致 \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34; } } --- apiVersion: apps/v1 kind: DaemonSet metadata: name: kube-flannel-ds namespace: kube-system labels: tier: node app: flannel spec: selector: matchLabels: app: flannel template: metadata: labels: tier: node app: flannel spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/os operator: In values: - linux hostNetwork: true priorityClassName: system-node-critical tolerations: - operator: Exists effect: NoSchedule serviceAccountName: flannel initContainers: - name: install-cni image: quay.io/coreos/flannel:v0.13.1-rc2 command: - cp args: - -f - /etc/kube-flannel/cni-conf.json - /etc/cni/net.d/10-flannel.conflist volumeMounts: - name: cni mountPath: /etc/cni/net.d - name: flannel-cfg mountPath: /etc/kube-flannel/ containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.13.1-rc2 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr resources: requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; limits: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;50Mi\u0026#34; securityContext: privileged: false capabilities: add: [\u0026#34;NET_ADMIN\u0026#34;, \u0026#34;NET_RAW\u0026#34;] env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace volumeMounts: - name: run mountPath: /run/flannel - name: flannel-cfg mountPath: /etc/kube-flannel/ volumes: - name: run hostPath: path: /run/flannel - name: cni hostPath: path: /etc/cni/net.d - name: flannel-cfg configMap: name: kube-flannel-cfg kubeadm_join.sh #  #!/bin/bash  sudo kubeadm join ip:6443 --token wxjhuh.z9ru6bbz990m7v0i --discovery-token-ca-cert-hash sha256:8809f1cc27401d704faa104008a48034b51e8e5ef8f4a8f33f0e267db0124a2f 一些问题 #  1.k8s: reset 后出现 coredns 一直处于 ContainerCreating 状态\n 解决方法 1\n2.k8s: connet to localhost:8080 failed\nkubeadm init 成功后，执行sudo kubectl apply -f kube-flannel.yml，出现错误：\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?  issue\n原来是因为没有执行export KUBECONFIG=/etc/kubernetes/admin.conf命令的时候使用 kubectl 是不能在前面加 sudo 的。\n所以，直接执行kubectl apply -f kube-flannel.yml就正常了。\n基于 wsl2 搭建 #   基于 wsl2 搭建\nDeploy #  k8s: 部署应用之 deployment service pod\n问题 1 #  在创建 deployment 时，如果想从本地获取镜像，需要将 yaml 配置文件里的imagePullPolicy，镜像拉取机制，从Always改为IfNotPresent。\n问题 2 #  怎么让 pod 里的应用访问到非集群内部的本机数据库实例呢？\n通过新建一个 service，指定为type:ExternalName或Endpoints，并把想要访问的数据库实例信息写到配置上。\n 参照 1\n 参照 2\n 参照 3\n 参照 4\n 参照 5\n解决：在本机的数据库貌似不能访问到，只好在另外的虚拟机上安装 db，然后配置该虚拟机的 ip 地址到 endpoint。\n使用 service 将应用暴露到公网 #  # 针对deployment hello-world以NodePort方式、example-service名称、端口是port-value暴露应用到公网 kubectl expose deployment hello-world --type=NodePort --name=example-service -- port=port-value 通过公网访问时，需要先使用kubectl describe svc拿到 example-service 服务的 NodePort 值，再结合机器自身的 ip 地址，组成 ip:NodePort 值访问，如： http://192.168.9.16:32256/。\njd@wnode1:~/Project/jdnote$ kubectl describe svc Name: jdnote-server-service Namespace: default Labels: app=jdnote-server Annotations: \u0026lt;none\u0026gt; Selector: app=jdnote-server Type: NodePort IP Families: \u0026lt;none\u0026gt; IP: 192.168.1.198 IPs: 192.168.1.198 Port: \u0026lt;unset\u0026gt; 8890/TCP TargetPort: 8890/TCP NodePort: \u0026lt;unset\u0026gt; 32256/TCP # 这个值作为端口 Endpoints: 10.46.129.11:8890 Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; jd@wnode1:~/Project/jdnote$ ip a 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: enp0s3: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:cf:aa:13 brd ff:ff:ff:ff:ff:ff inet 192.168.9.16/24 brd 192.168.9.255 scope global dynamic enp0s3 # 192.168.9.16作为ip valid_lft 9843sec preferred_lft 9843sec inet6 fe80::a00:27ff:fecf:aa13/64 scope link valid_lft forever preferred_lft forever  参照\n 网络介绍\nk8s: ingress #   k8s: ingress\n让外网能访问到 k8s 里的应用。\n单个服务：\napiVersion: networking.k8s.io/v1 kind: Ingress # kind必须指定为ingress metadata: name: minimal-ingress # 名称 annotations: # 注解 nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: # 规则 - http: paths: # 路径 - path: /testpath pathType: Prefix backend: # 重定向到service service: name: test # 必须存在test service port: number: 80 # service的端口 多个服务：\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-wildcard-host spec: rules: - host: \u0026#34;foo.bar.com\u0026#34; # 域名，表示该域名的流量将被转到配置的service http: paths: - pathType: Prefix path: \u0026#34;/bar\u0026#34; backend: service: name: service1 port: number: 80 - host: \u0026#34;*.foo.com\u0026#34; http: paths: - pathType: Prefix path: \u0026#34;/foo\u0026#34; backend: service: name: service2 port: number: 80 查看：kubectl describe ingress minimal-ingress\n 可选 ingress 控制器\nIngress 只是 Kubernetes 中的一种配置信息；Ingress Controller 才是监听 80/443 端口，并根据 Ingress 上配置的路由信息执行 HTTP 路由转发的组件。\n traefik 提供的 k8s 控制器\n 使用 traefik\nlist-watch 模式 #   来源\n 谈谈 List-Watch 的设计理念\n当设计优秀的一个异步消息的系统时，对消息机制有至少如下四点要求：\n 消息可靠性\n消息实时性\n消息顺序性\n高性能\n首先消息必须是可靠的，list 和 watch 一起保证了消息的可靠性，避免因消息丢失而造成状态不一致场景。\n具体而言，list API 可以查询当前的资源及其对应的状态(即期望的状态)，客户端通过拿期望的状态和实际的状态进行对比，纠正状态不一致的资源。Watch API 和 apiserver 保持一个长链接，接收资源的状态变更事件并做相应处理。如果仅调用 watch API，若某个时间点连接中断，就有可能导致消息丢失，所以需要通过 list API 解决消息丢失的问题。从另一个角度出发，我们可以认为 list API 获取全量数据，watch API 获取增量数据。虽然仅仅通过轮询 list API，也能达到同步资源状态的效果，但是存在开销大，实时性不足的问题。\n 消息必须是实时的，list-watch 机制下，每当 apiserver 的资源产生状态变更事件，都会将事件及时的推送给客户端，从而保证了消息的实时性。\n消息的顺序性也是非常重要的，在并发的场景下，客户端在短时间内可能会收到同一个资源的多个事件，对于关注最终一致性的 K8S 来说，它需要知道哪个是最近发生的事件，并保证资源的最终状态如同最近事件所表述的状态一样。K8S 在每个资源的事件中都带一个 resourceVersion 的标签，这个标签是递增的数字，所以当客户端并发处理同一个资源的事件时，它就可以对比 resourceVersion 来保证最终的状态和最新的事件所期望的状态保持一致。\nList-watch 还具有高性能的特点，虽然仅通过周期性调用 list API 也能达到资源最终一致性的效果，但是周期性频繁的轮询大大的增大了开销，增加 apiserver 的压力。而 watch 作为异步消息通知机制，复用一条长链接，保证实时性的同时也保证了性能。\n 调试 #  golang: dlv 调试 k8s 容器里的 go 进程\n使用容器 exec 进行调试 #  如果 容器镜像 包含调试程序(dlv, gdb)， 比如从 Linux 和 Windows 操作系统基础镜像构建的镜像，你可以使用 kubectl exec 命令 在特定的容器中运行一些命令：\nkubectl exec ${POD_NAME} -c ${CONTAINER_NAME} -- ${CMD} ${ARG1} ${ARG2} ... ${ARGN}\n 说明： -c ${CONTAINER_NAME} 是可选择的。如果 Pod 中仅包含一个容器，就可以忽略它。\n 例如，要查看正在运行的 Cassandra pod 中的日志，可以运行：\nkubectl exec cassandra -- cat /var/log/cassandra/system.log\n你可以在 kubectl exec 命令后面加上 -i 和 -t 来运行一个连接到你的终端的 Shell，比如：\nkubectl exec -it cassandra -- sh\n 参照 1\n 参照 2\n尝试 #  $ kubectl exec -it skm-7fb6bd989c-kg9xj -n skm-system -- sh # 使用 kubectl 进入容器的 sh 界面 sh-4.4# dlv exec /root/Project/rancher/bin/rancher # exec 报错：版本太旧 Version of Delve is too old for this version of Go (maximum supported version 1.13, suppress this error with --check-go-version=false) sh-4.4# dlv attach /root/Project/rancher/bin/rancher # attach 需要指定 pid Invalid pid: /root/Project/rancher/bin/rancher sh-4.4# dlv attach 653 # 通过 ps aux 拿到 pid Type \u0026#39;help\u0026#39; for list of commands. (dlv)  dlv 使用\n"},{"id":1,"href":"/posts/2021/12/ddia/","title":"数据密集型应用设计","section":"12","content":"第一章 可靠性、可扩展性、可维护性 #  数据密集型应用和计算密集型应用\n 现今很多应用程序都是 数据密集型（data-intensive） 的，而非 计算密集型（compute-intensive） 的。因此CPU很少成为这类应用的瓶颈，更大的问题通常来自数据量、数据复杂性、以及数据的变更速度。\n 数据密集型应用通常由标准组件构建而成，标准组件提供了很多通用的功能；例如，许多应用程序都需要：\n 存储数据，以便自己或其他应用程序之后能再次找到 （数据库（database））\n记住开销昂贵操作的结果，加快读取速度（缓存（cache））\n允许用户按关键字搜索数据，或以各种方式对数据进行过滤（搜索索引（search indexes））\n向其他进程发送消息，进行异步处理（流处理（stream processing））\n定期处理累积的大批量数据（批处理（batch processing））\n 对应可选的组件在我映像中可以有：\n数据库：mysql, postgresql\n缓存: redis, memcached\n搜索索引: elastic search, sonic, redis search\n流处理: kafka, redis stream\n批处理: linux cron, golang timer\n  使用较小的通用组件创建了一个全新的、专用的数据系统。\n 如何衡量一个系统的好坏 #  设计数据系统或服务时可能会遇到很多棘手的问题，例如：当系统出问题时，如何确保数据的正确性和完整性？当部分系统退化降级时，如何为客户提供始终如一的良好性能？当负载增加时，如何扩容应对？什么样的 API 才是好的 API？\n  可靠性（Reliability）\n 系统在困境（adversity）（硬件故障、软件故障、人为错误）中仍可正常工作（正确完成功能，并能达到期望的性能水准）。\n故障通常定义为系统的一部分状态偏离其标准，而失效则是系统作为一个整体停止向用户提供服务。故障的概率不可能降到零，因此最好设计容错机制以防因故障而导致失效。\n硬件错误的解决：为了减少系统的故障率，第一反应通常都是增加单个硬件的冗余度，例如：磁盘可以组建 RAID，服务器可能有双路电源和热插拔 CPU，数据中心可能有电池和柴油发电机作为后备电源，某个组件挂掉时冗余组件可以立刻接管。\n软件错误的解决：仔细考虑系统中的假设和交互；彻底的测试；进程隔离；允许进程崩溃并重启；测量、监控并分析生产环境中的系统行为。\n人为错误的解决：\n   可扩展性（Scalability）\n 有合理的办法应对系统的增长（数据量、流量、复杂性）\n   可维护性（Maintainability）\n 许多不同的人（工程师、运维）在不同的生命周期，都能高效地在系统上工作（使系统保持现有行为，并适应新的应用场景）。\n   从人的角度看：可靠就是能共困苦，同富贵；可扩展就是学习能力强，心胸广阔；可维护就是对人对己无偏见、无特例。\n2PC（两阶段提交） #  2PC，two-phase commit，两阶段提交。\n 一种用于实现跨多个节点的原子事务提交的算法，即确保所有节点提交或所有节点中止。\n   2PC 使用一个通常不会出现在单节点事务中的新组件：协调者（coordinator）（也称为事务管理器（transaction manager））。\n正常情况下，2PC 事务以应用在多个数据库节点上读写数据开始。我们称这些数据库节点为参与者（participants）。\n 当应用准备提交时，协调者开始阶段 1 ：它发送一个准备（prepare）请求到每个节点，询问它们是否能够提交。然后协调者会跟踪参与者的响应：\n如果所有参与者都回答“是”，表示它们已经准备好提交，那么协调者在阶段 2 发出提交（commit）请求，然后提交真正发生。\n如果任意一个参与者回复了“否”，则协调者在阶段 2 中向所有节点发送中止（abort）请求。\n  那如果在阶段 2 有事务提交失败了呢？\n 在两阶段提交的情况下，准备(prepare 阶段)请求和提交(commit 阶段)请求当然也可以轻易丢失。 2PC 又有什么不同呢？\n  第十一章 流处理 #   先回忆了批处理的特点：即输入是有界的，即已知和有限的大小，所以批处理知道它何时完成输入的读取。\n实际上，很多数据是无界限的，因为它随着时间的推移而逐渐到达：你的用户在昨天和今天产生了数据，明天他们将继续产生更多的数据。除非你停业，否则这个过程永远都不会结束，所以数据集从来就不会以任何有意义的方式“完成”。\n因此，批处理程序必须将数据人为地分成固定时间段的数据块，例如，在每天结束时处理一天的数据，或者在每小时结束时处理一小时的数据。\n日常批处理中的问题是，输入的变更只会在一天之后的输出中反映出来，这对于许多急躁的用户来说太慢了。为了减少延迟，我们可以更频繁地运行处理 —— 比如说，在每秒钟的末尾 —— 或者甚至更连续一些，完全抛开固定的时间切片，当事件发生时就立即进行处理，这就是流处理（stream processing）背后的想法。\n 在本章中，我们将把事件流（event stream）视为一种数据管理机制：无界限，增量处理，与上一章中批量数据相对应。\n 原则上讲，文件或数据库就足以连接生产者和消费者：生产者将其生成的每个事件写入数据存储，且每个消费者定期轮询数据存储，检查自上次运行以来新出现的事件。这实际上正是批处理在每天结束时处理当天数据时所做的事情。\n但当我们想要进行低延迟的连续处理时，如果数据存储不是为这种用途专门设计的，那么轮询开销就会很大。轮询的越频繁，能返回新事件的请求比例就越低，而额外开销也就越高。相比之下，最好能在新事件出现时直接通知消费者。\n数据库在传统上对这种通知机制支持的并不好，关系型数据库通常有 触发器（trigger） ，它们可以对变化作出反应（如，插入表中的一行），但是它们的功能非常有限，并且在数据库设计中有些后顾之忧。相应的是，已经开发了专门的工具来提供事件通知。\n 两个问题：\n  如果生产者发送消息的速度比消费者能够处理的速度快会发生什么？\n  如果节点崩溃或暂时脱机，会发生什么情况？ —— 是否会有消息丢失？\n  命令和事件 #   事件溯源的哲学是仔细区分事件（event）和命令（command）。\n当来自用户的请求刚到达时，它一开始是一个命令：在这个时间点上它仍然可能可能失败，比如，因为违反了一些完整性条件。应用必须首先验证它是否可以执行该命令。\n如果验证成功并且命令被接受，则它变为一个持久化且不可变的事件。\n在事件生成的时刻，它就成为了事实（fact）。即使客户稍后决定更改或取消预订，他们之前曾预定了某个特定座位的事实仍然成立，而更改或取消是之后添加的单独的事件。\n "},{"id":2,"href":"/posts/2021/12/burn_cpu_use_golang/","title":"burn cpu use golang","section":"12","content":"虚假的 burn #  package main func fakeBurn() { for { } } 真正的 burn #  package main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; ) var ( numBurn int updateInterval int ) func cpuBurn() { for { for i := 0; i \u0026lt; 2147483647; i++ { } // Gosched yields the processor, allowing other goroutines to run. It does not suspend the current goroutine, so execution resumes automatically.  // Gosched让当前goroutine让出处理器，从而使得其它goroutine可以运行。它不会挂起/暂停当前的goroutine，它会自动恢复执行。  runtime.Gosched() } } func init() { flag.IntVar(\u0026amp;numBurn, \u0026#34;n\u0026#34;, 0, \u0026#34;number of cores to burn (0 = all)\u0026#34;) flag.IntVar(\u0026amp;updateInterval, \u0026#34;u\u0026#34;, 10, \u0026#34;seconds between updates (0 = don\u0026#39;t update)\u0026#34;) flag.Parse() if numBurn \u0026lt;= 0 { numBurn = runtime.NumCPU() } } func main() { runtime.GOMAXPROCS(numBurn) fmt.Printf(\u0026#34;Burning %d CPUs/cores\\n\u0026#34;, numBurn) for i := 0; i \u0026lt; numBurn; i++ { go cpuBurn() } // 一直执行，区别是其中一个会定期打印，另一个不会打印  if updateInterval \u0026gt; 0 { t := time.Tick(time.Duration(updateInterval) * time.Second) for secs := updateInterval; ; secs += updateInterval { \u0026lt;-t fmt.Printf(\u0026#34;%d seconds\\n\u0026#34;, secs) } } else { select {} // wait forever  } } "},{"id":3,"href":"/posts/2021/12/docker_compose_extra_host/","title":"docker compose使用extra host让容器访问主机服务","section":"12","content":" 首发于：简单博客\ndocker compose 如何访问主机服务 #  docker compose 里面的容器怎么访问主机自身起的服务呢？\n 20.10.0 版本在 linux 新增 host.docker.internal 支持： docker run -it --add-host=host.docker.internal:host-gateway alpine cat /etc/hosts\n127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopback fe00::0 ip6-localnet ff00::0 ip6-mcastprefix ff02::1 ip6-allnodes ff02::2 ip6-allrouters 172.17.0.1 host.docker.internal # --add-host的作用就是添加了这行到/etc/hosts 172.17.0.3 cb0565ceea26  相关提交\n这个 add-host 的意思是告诉容器，容器对域名 host.docker.internal 的访问都将转发到 host-gateway 去。\n也就是容器内部访问这个域名 host.docker.internal 时，就会访问到对应的主机上的 host-gateway 地址。\n从而达到容器访问主机上服务的效果。\n那么，这个 add-host 怎么用在 compose 上呢？\n 在 build 里使用 extra_hosts\nversion: \u0026#34;2.3\u0026#34; # 因为某个bug的存在，只能用version2，不能用version3 services: tmp: build: context: . extra_hosts: # 配置extra_hosts - \u0026#34;host:IP\u0026#34; command: -kIL https://host tty: true stdin_open: true  docker compose 配置中文说明\n 参照\n测试：\ndocker-compose --version docker-compose version 1.29.2, build 5becea4c 新建一个服务，在主机上运行；\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { handler := http.HandlerFunc(func(resp http.ResponseWriter, req *http.Request) { fmt.Println(\u0026#34;hi\u0026#34;) resp.Write([]byte(\u0026#34;hello\u0026#34;)) }) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, handler); err != nil { panic(err) } } 新建 compose，里面也起一个服务，这个服务需要访问上述的主机服务；\nversion: \u0026#34;2.3\u0026#34; # version改为3.3也可以 services: server: image: curlimages/curl command: curl http://host.docker.internal:8080 extra_hosts: - \u0026#34;host.docker.internal:host-gateway\u0026#34; 在终端访问容器服务，容器服务访问主机服务，如果能正常执行，则表示完成。\n执行docker-compose up，能看到请求成功。\n"},{"id":4,"href":"/posts/2021/12/dbeaver/","title":"数据库管理工具之dbeaver","section":"12","content":" dbeaver: github\n 下载页\n面向开发者、SQL 编程人员、数据库管理员和分析人员的免费的多平台数据库工具。 支持任何已有 JDBC 驱动的数据库（基本上是任何数据库）。商业版本还额外支持非 JDBC 数据源，比如：MongoDB, Cassandra, Couchbase, Redis, BigTable, DynamoDB 等。\n拥有的特性：元数据编辑、SQL 编辑、富文本编辑、ER 图、数据导出/导入/转译、SQL 执行计划等。 基于 Eclipse 平台。 使用插件架构，为以下数据库提供额外功能：MySQL/MariaDB, PostgreSQL, Greenplum, Oracle, DB2 LUW, Exasol, SQL Server, Sybase/SAP ASE, SQLite, Firebird, H2, HSQLDB, Derby, Teradata, Vertica, Netezza, Informix 等。\n Free multi-platform database tool for developers, SQL programmers, database administrators and analysts. Supports any database which has JDBC driver (which basically means - ANY database). Commercial versions also support non-JDBC datasources such as MongoDB, Cassandra, Couchbase, Redis, BigTable, DynamoDB, etc. You can find the list of all databases supported in commercial versions here.\nHas a lot of features including metadata editor, SQL editor, rich data editor, ERD, data export/import/migration, SQL execution plans, etc. Based on Eclipse platform. Uses plugins architecture and provides additional functionality for the following databases: MySQL/MariaDB, PostgreSQL, Greenplum, Oracle, DB2 LUW, Exasol, SQL Server, Sybase/SAP ASE, SQLite, Firebird, H2, HSQLDB, Derby, Teradata, Vertica, Netezza, Informix, etc.\n "},{"id":5,"href":"/posts/2021/12/domain/","title":"Domain-oriented development","section":"12","content":"面向领域开发。\n将业务复杂度和技术复杂度分开，逐个击破。\n分离领域，各司其职。\n降低复杂度，容易测试。\nDDD 尝试 #  order.go:\npackage domain import ( \u0026#34;crypto/rand\u0026#34; \u0026#34;math/big\u0026#34; \u0026#34;github.com/pkg/errors\u0026#34; ) // 关键词：用户、店铺、商品、订单 // // 场景描述：店铺展示商品，其价格为P、库存为N，用户（余额为Y）看到商品觉得合适，于是下单购买B个； // 购买前，用户余额Y必须不小于P，商品库存N不小于B；购买后，用户余额减少P，库存减少B； // // 先不考虑并发情况，建立此时的领域模型  type User struct { name string // 名称  phone string // 电话  balance Money // 余额 } type Shop struct { name string // 名称  addr string // 地址 } type Product struct { name string // 名称  price Money // 价格  stock int // 库存  ownShop *Shop // 所属商铺 } type Order struct { name string // 名称  user *User // 用户  product *Product // 商品 } type Money int func NewUser(name, phone string, bal Money) *User { return \u0026amp;User{ name: name, phone: phone, balance: bal, } } func (u *User) Balance() Money { return u.balance } func (u *User) DeductBalance(amount Money) { if u.balance \u0026lt; amount { panic(\u0026#34;not enough money\u0026#34;) } u.balance -= amount } func NewShop(name, addr string) *Shop { return \u0026amp;Shop{ name: name, addr: addr, } } func NewProduct(name string, price Money, stock int, shop *Shop) *Product { return \u0026amp;Product{ name: name, price: price, stock: stock, ownShop: shop, } } func (p *Product) Stock() int { return p.stock } func (p *Product) DeductStock(c int) { if p.stock \u0026lt; c { panic(\u0026#34;not enough stock\u0026#34;) } p.stock -= c } // NewOrder 用户对商品下单c个 func NewOrder(user *User, product *Product, c int) *Order { name, err := GenerateRandomString(12) if err != nil { panic(err) } user.DeductBalance(product.price * Money(c)) product.DeductStock(c) return \u0026amp;Order{ name: name, user: user, product: product, } } func (o *Order) User() *User { return o.user } func (o *Order) Product() *Product { return o.product } // GenerateRandomString 随机字符串包含有数字和大小写字母 func GenerateRandomString(n int) (string, error) { const ( letters = \u0026#34;0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\u0026#34; ) return generate(n, letters) } func generate(n int, letters string) (string, error) { ret := make([]byte, n) for i := 0; i \u0026lt; n; i++ { num, err := rand.Int(rand.Reader, big.NewInt(int64(len(letters)))) if err != nil { return \u0026#34;\u0026#34;, errors.WithStack(err) } ret[i] = letters[num.Int64()] } return string(ret), nil } order_test.go:\npackage domain_test import ( \u0026#34;testing\u0026#34; \u0026#34;github.com/donnol/blog/demo/go/domain\u0026#34; ) func TestNewOrder(t *testing.T) { type args struct { user *domain.User product *domain.Product c int } tests := []struct { name string args args want *domain.Order }{ {name: \u0026#34;\u0026#34;, args: args{ user: domain.NewUser(\u0026#34;jd\u0026#34;, \u0026#34;123\u0026#34;, 10000), product: domain.NewProduct(\u0026#34;树莓派\u0026#34;, 1000, 10, domain.NewShop(\u0026#34;a shop\u0026#34;, \u0026#34;zhongshan\u0026#34;)), c: 1, }, want: nil}, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { if got := domain.NewOrder( tt.args.user, tt.args.product, tt.args.c, ); got.User().Balance() != 9000 || got.Product().Stock() != 9 { t.Logf(\u0026#34;user: %+v, product: %+v\\n\u0026#34;, got.User(), got.Product()) t.Errorf(\u0026#34;NewOrder() = %v, want %v\u0026#34;, got, tt.want) } }) } } "},{"id":6,"href":"/posts/aboutme/about-me/","title":"关于我","section":"Posts","content":"我自己 #  一名默默无闻的打工仔，做着后端开发里的一点微末工作。\n当然怀抱着一夜暴富的乐观想法，希望每天平安快乐。\n也对一些事情无比憎恶，时刻提醒自己适可宜止。\n竟然也对各种球类运动感冒，偶尔踢出一片天地或抽出一道弧线。\n我工作 #  专注于微服务、云原生、区块链领域，以快准稳著称，用想象力和创造力打造一片天地。\n在计算、传输、存储方面略有心得，日复一日地析算法、理源码、画图文、成系统。\n"},{"id":7,"href":"/posts/2021/12/github_action_deploy_hugo_blog/","title":"github action deploy hugo blog","section":"12","content":"why #  为了将视线保持在文章上，减少构建和发布的时间占用。\nwhat #  github action是GitHub推出的持续集成/持续部署工具，只需要在项目中添加workflow.yml配置文件，在其中配置好任务、工作、步骤等，即可在指定动作发生时自动触发编排好的动作。换言之，如果我们在我们的博客仓库里配置了自动将内容打包和发布的workflow.yml，那我们就可以把精力集中在文章的编写，然后轻轻地提交推送，即可完成博客地打包和发布，very easy and smooth。\nhow #  在github准备一个blog仓库，用于存放原始信息；再准备一个github page仓库，用于存放打包数据。\n其中github page仓库已开启page，可以通过github page设置的域名访问。\n 我的blog仓库\n 我的github page仓库\nworkflow #   这是我结合网络各位英豪所总结出来的一个workflow.yml配置文件\nname: blog # 做什么都好，别忘了先起个平凡（kuxuan）的名字 on: # 指定触发动作 push: # 动作是：git push branches: - main # 指定分支： main jobs: build-deploy: runs-on: ubuntu-latest # 基于ubuntu steps: - uses: actions/checkout@v2 # 切换分支：git checkout with: submodules: recursive  # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo # 博客所用的打包和部署工具 uses: peaceiris/actions-hugo@v2 with: hugo-version: latest - name: Build # 打包 run: hugo --minify --baseURL=https://donnol.github.io # 指定base url，确保构建出来的内容里的超链接都在它里面 - name: Deploy # 部署 uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }} # 这个key非常关键，一言两语很难讲清楚 external_repository: donnol/donnol.github.io # 我的github page所在的仓库 PUBLISH_BRANCH: main PUBLISH_DIR: ./public # 将本仓库的public目录下的内容提交到github page仓库 commit_message: ${{ github.event.head_commit.message }} # 提交信息 以铜为镜，可以正衣冠；以人为镜，可以明得失； 以史为镜，可以知兴替。\ndeploy key #    使用ssh-keygen生产一对非对称秘钥（包含有公钥、私钥）\n  在github page(我这里是donnol/donnol.github.io)的仓库的setting里的deploy里添加公钥\n  在blog仓库setting的secrets里添加私钥，注意命名必须是workflow里使用的名称(如上述：ACTIONS_DEPLOY_KEY)\n  Q\u0026amp;A #  遇到问题不要惊慌，阿Q怕的是强者，如果你示弱，结果可想而知。\n当然，实在搞不懂，也可以在issue里提问，本人不负责任地想回就回。\n温馨提示 #  如果想知道更详细的信息，请自行搜索关键词，网络大神比比皆是，学习资料处处有售，生活实践时时待你。\n"},{"id":8,"href":"/posts/2021/01/proxy-between-layer/","title":"go实现AOP","section":"01","content":"go实现AOP #  假设有store，从数据库获取数据，其中有方法IUserStore.GetByID，传入id参数，返回用户信息:\ntype IUserStore interface { GetByID(ctx context.Context, id int) (User, error) } 另外有service，刚好有用户id并且需要拿到用户信息，于是依赖了上述IUserStore：\ntype IUserSrv interface { CheckUser(ctx context.Context, id int) error // 获取用户信息，然后检查用户某些属性 } type userImpl struct { userStore IUserStore } func (impl userImpl) CheckUser(ctx context.Context, id int) error { user, err := impl.userStore.GetByID(ctx, id) if err != nil { return err } // 使用user数据做一些操作  _ = user } 上面所描述的是一个最简单的情况，如果我们要在userImpl.CheckUser里对impl.userStore.GetByID方法调用添加耗时统计，依然十分简单。\nfunc (impl userImpl) CheckUser(ctx context.Context, id int) error { begin := time.Now() user, err := impl.userStore.GetByID(ctx, id) if err != nil { return err } fmt.Println(time.Since(begin)) // 统计耗时  // 使用user数据做一些操作  _ = user } 但是，如果方法里调用的类似impl.userStore.GetByID的方法非常之多，逻辑非常之复杂时，这样一个一个的添加，必然非常麻烦、非常累。\n这时，如果有一个层间代理能帮我们拦截store的方法调用，在调用前后添加上耗时统计，势必能大大提升我们的工作效率。\n比如：\nfunc Around(f func(args []interface{}) []interface{}, args []interface{}) []interface{} { begin := time.Now() r := f(args) fmt.Println(time.Since(begin)) // 统计耗时  return r } 这只是一个简单的包装函数，怎么能将它与上面的接口联系到一起呢？\n有兴趣的话，可以看这里的实现 #  可以看到，主要的方法是Around(provider interface{}, mock interface{}, arounder Arounder) interface{}， 其中provider参数是类似NewXXX() IXXX的函数，而mock是IXXX接口的一个实现，最后的Arounder是 拥有方法Around(pctx ProxyContext, method reflect.Value, args []reflect.Value) []reflect.Value的接口。\n这里的示例 #  可以看到，mock结构是长这样的：\ntype UserMock struct { AddFunc func(name string) int GetHelper func(id int) string `method:\u0026#34;Get\u0026#34;` // 表示这个字段关联的方法是Get \tGetContextFunc func(ctx context.Context, id int) string } 所以，为了提升开发效率，我还写了一个 工具，用来根据接口生成相应的mock结构体。\n"},{"id":9,"href":"/posts/2021/01/hugo-blog/","title":"hugo搭建博客","section":"01","content":"操作 #    安装hugo。\n  使用hugo新建项目\n  添加主题\n  启动博客\n  "},{"id":10,"href":"/posts/2020/12/go-ctx/","title":"go ctx","section":"12","content":"ctx #  1.why\ngoroutine号称百万之众，互相之间盘根错节，难以管理控制。为此，必须提供一种机制来管理控制它们。\n各自为战 #  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { // start first  go func() { fmt.Println(1) }() // start second  go func() { fmt.Println(2) }() time.Sleep(time.Second) } 万法归一 #  package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { wg := new(sync.WaitGroup) // start first  wg.Add(1) go func() { defer wg.Done() fmt.Println(1) }() // start second  wg.Add(1) go func() { defer wg.Done() fmt.Println(2) }() wg.Wait() } 可以看到使用waitgroup可以控制多个goroutine必须互相等待，直到最后一个完成才会全部完成。\n明修栈道暗度陈仓 #  package main import ( \u0026#34;fmt\u0026#34; ) func main() { ch1 := make(chan int) ch2 := make(chan int) // start first  go func() { fmt.Println(1) \u0026lt;-ch2 ch1 \u0026lt;- 1 }() ch3 := make(chan int) // start second  go func() { fmt.Println(2) ch2 \u0026lt;- 2 \u0026lt;-ch1 // escape  ch3 \u0026lt;- 3 }() n := \u0026lt;-ch3 fmt.Println(n) } 使用chan的话，可以实现goroutine之间的消息同步\n2.what\n Package context defines the Context type, which carries deadlines, cancellation signals, and other request-scoped values across API boundaries and between processes.\n\u0026ndash; 提供标准库context，定义了Context类型，带有限期、取消信息和其它请求域里的跨API边界和进程间的值。\n3.how\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { var n time.Duration = 2 now := time.Now() ctx, cancel := context.WithDeadline(context.Background(), now.Add(time.Second*n)) _ = cancel fmt.Println(now) // start first  go func(ctx context.Context) { select { case \u0026lt;-ctx.Done(): } fmt.Println(time.Now(), 1) }(ctx) // start second  go func(ctx context.Context) { select { case \u0026lt;-ctx.Done(): } fmt.Println(time.Now(), 2) }(ctx) time.Sleep(time.Second * (n - 1)) fmt.Println(time.Now()) // 一秒钟之后取消的话，两个goroutine会在取消后马上执行；如果等到时间到期了，就会在两秒后执行；  // cancel()  // fmt.Println(time.Now())  time.Sleep(time.Second * (n + 1)) } 4.others\n"},{"id":11,"href":"/posts/2021/01/pstree/","title":"pstree进程树及说明","section":"01","content":"pstree进程树及说明 #   "},{"id":12,"href":"/posts/2021/07/linux-epoll/","title":"Linux Epoll","section":"07","content":"linux epoll #   wiki\n 手册\nwhy #  what #  Linux内核的可扩展I/O事件通知机制。\n于Linux 2.5.44首度登场，它设计目的旨在取代既有POSIX select(2)与poll(2)系统函数，让需要大量操作文件描述符的程序得以发挥更优异的性能（举例来说：旧有的系统函数所花费的时间复杂度为O(n)，epoll的时间复杂度O(log n)）。epoll 实现的功能与 poll 类似，都是监听多个文件描述符上的事件。\nhow #  epoll 通过使用红黑树(RB-tree)搜索被监控的文件描述符(file descriptor)。\n在 epoll 实例上注册事件时，epoll 会将该事件添加到 epoll 实例的红黑树上并注册一个回调函数，当事件发生时会将事件添加到就绪链表中。\nint epoll_create(int size); 在内核中创建epoll实例并返回一个epoll文件描述符。\nint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); 向 epfd 对应的内核epoll 实例添加、修改或删除对 fd 上事件 event 的监听。op 可以为 EPOLL_CTL_ADD, EPOLL_CTL_MOD, EPOLL_CTL_DEL 分别对应的是添加新的事件，修改文件描述符上监听的事件类型，从实例上删除一个事件。如果 event 的 events 属性设置了 EPOLLET flag，那么监听该事件的方式是边缘触发。\nint epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout); 当 timeout 为 0 时，epoll_wait 永远会立即返回。而 timeout 为 -1 时，epoll_wait 会一直阻塞直到任一已注册的事件变为就绪。当 timeout 为一正整数时，epoll 会阻塞直到计时 timeout 毫秒终了或已注册的事件变为就绪。因为内核调度延迟，阻塞的时间可能会略微超过 timeout 毫秒。\n触发模式 #  epoll提供边沿触发及状态触发模式。\n在边沿触发模式中，epoll_wait仅会在新的事件首次被加入epoll队列时返回；在状态触发模式下，epoll_wait在事件状态未变更前将不断被触发。状态触发模式是默认的模式。\n状态触发模式与边沿触发模式有读和写两种情况，我们先来考虑读的情况。假设我们注册了一个读事件到epoll实例上，epoll实例会通过epoll_wait返回值的形式通知我们哪些读事件已经就绪。简单地来说，在状态触发模式下，如果读事件未被处理，该事件对应的内核读缓冲器非空，则每次调用epoll_wait时返回的事件列表都会包含该事件，直到该事件对应的内核读缓冲器为空为止。而在边沿触发模式下，读事件就绪后只会通知一次，不会反复通知。\n然后我们再考虑写的情况。状态触发模式下，只要文件描述符对应的内核写缓冲器未满，就会一直通知可写事件。而在边沿触发模式下，内核写缓冲器由满变为未满后，只会通知一次可写事件。\n举例来说，倘若有一个已经于epoll注册之流水线接获资料，epoll_wait将返回，并发出资料读取的信号。现假设缓冲器的资料仅有部分被读取并处理，在level-triggered(状态触发)模式下，任何对epoll_wait之调用都将即刻返回，直到缓冲器中的资料全部被读取；然而，在edge-triggered(边缘触发)的情境下，epoll_wait仅会于再次接收到新资料(亦即，新资料被写入流水线)时返回。\n边沿触发模式 #  边沿触发模式使得程序有可能在用户态缓存 IO 状态。nginx 使用的是边沿触发模式。\n文件描述符有两种情况是推荐使用边沿触发模式的。\n read 或者 write 系统调用返回了 EAGAIN。 非阻塞的文件描述符。  可能的缺陷：\n如果 IO 空间很大，你要花很多时间才能把它一次读完，这可能会导致饥饿。举个例子，假设你在监听一个文件描述符列表，而某个文件描述符上有大量的输入（不间断的输入流），那么你在读完它的过程中就没空处理其他就绪的文件描述符。（因为边沿触发模式只会通知一次可读事件，所以你往往会想一次把它读完。）一种解决方案是，程序维护一个就绪队列，当 epoll 实例通知某文件描述符就绪时将它在就绪队列数据结构中标记为就绪，这样程序就会记得哪些文件描述符等待处理。Round-Robin 循环处理就绪队列中就绪的文件描述符即可。\n如果你缓存了所有事件，那么一种可能的情况是 A 事件的发生让程序关闭了另一个文件描述符 B。但是内核的 epoll 实例并不知道这件事，需要你从 epoll 删除掉。\n"},{"id":13,"href":"/posts/2021/12/etcd/","title":"Etcd","section":"12","content":"etcd #  raft #  介绍 #   由多个节点组成的集群维护着一个可复制状态机的协议。通过复制日志来保持状态机的同步。 可理解的共识算法\n状态机以消息为输入。消息可以是一个本地定时器更新，或一条网络消息。输出一个3元结构：[]Messages, []LogEntries, NextState，分别是消息列表、日志条目列表、下个状态。同样状态的状态机，在相同输入时总是输出相同结果。\n 插曲 #  人、联系、共识\n人生下来，触摸着这个世界的人和物，做着或有趣或无聊的事，建立起或浅或深的联系。\n当两个人面对面时，就某个想法达成一致或不一致，非常容易。\n如果两个人不是面对面呢？\n如果不只两个人，同坐在祠堂里呢？\n如果不止两个人，还分散在不同地点呢？\n那么，为什么要达成共识呢？\n因为有些事必须达成共识才能执行，比如，两个人双向奔赴。\n如果彼此异心，一个向东，一个往南，事情就办不成了。\n所以，共识是大伙成事的前提。\n共识，除了就某件事所要达成的结果，也要考虑所使用的方法。\n有可能是步步为营，走一步算一步，也就是每走一步再就下一步达成共识。\n也有可能是，一次性就接下来的几步均达成共识，然后各自执行。\nmessage type #  // For description of different message types, see: // https://pkg.go.dev/go.etcd.io/etcd/raft/v3#hdr-MessageType type MessageType int32 const ( // 选举时使用；  // 如果节点是一个follower或candidate，它在选举超时前没有收到任何心跳，它就回传递MsgHup消息给它自己的Step方法，然后成为（或保持）一个candidate从而开启一个新的选举 \tMsgHup MessageType = 0 // 一个内部类型，它向leader发送一个类型为“MsgHeartbeat”的心跳信号  // 如果节点是一个leader，raft里的tick函数将会是“tickHeartbeat”，触发leader周期性地发送“MsgHeartbeat”消息给它的followers \tMsgBeat MessageType = 1 // 提议往它的日志条目里追加数据；  // 这是一个特别的类型，由follower反推提议给leader（正常是leader提议，follower执行）；  // 发给leader的话，leader调用“appendEntry”方法追加条目到它的日志里，然后调用“bcastAppend”方法发送这些条目给它的远端节点；  // 发给candidate的话，它们直接丢弃该消息  // 发给follower的话，follower会将消息存储到它们的信箱里。会把发送者的id一起存储，然后转发给leader。 \tMsgProp MessageType = 2 // 包含了要复制的日志条目  // leader调用“bcastAppend”（里面调用“sendAppend”），发送“一会要被复制的日志”消息；  // 当candidate收到消息后，在它的Step方法里，它马上回退为follower，因为这条消息表明已经存在一个有效leader了。  // candidate和follower均会返回一条“MsgAppResp”类型消息以作响应。 \tMsgApp MessageType = 3 // 调用“handlerAppendEntries”方法 \tMsgAppResp MessageType = 4 // 请求集群中的节点给自己投票；  // 当节点是follower或candidate，并且它们的Step方法收到了“MsgHup”消息，节点调用“campaign”方法去提议自己成为一个leader。一旦“campaign”方法被调用，节点成为candidate，并发送“MsgVote”给集群中的远端节点请求投票。  // 当leader或candidate的Step方法收到该消息，并且消息的Term比它们的Term小，“MsgVote”将被拒绝。  // 当leader或candidate收到的消息的Term要更大时，它会回退为follower。  // 当follower收到该消息，仅当发送者的最后的term比“MsgVote”的term要大，或发送者的最后term等于“MsgVote”的term（但发送者的最后提交index大于等于follower的）， \tMsgVote MessageType = 5 // 投票响应；  // 当candidate收到后，它会统计选票，如果大于majority（quorum），它成为leader并调用“bcastAppend”。如果candidate收到大量的否决票，它将回退到follower \tMsgVoteResp MessageType = 6 // 请求安装一个快照消息；  // 当一个节点刚成为leader，或者leader收到了“MsgProp”消息，它调用“bcastAppend”方法（里面再调用“sendAppend”）方法到每个follower。在“sendAppend”方法里，如果一个leader获取term或条目失败了，leader通过\u0026#34;MsgSnap\u0026#34;消息请求快照。 \tMsgSnap MessageType = 7 // leader发送心跳；  // 当candidate收到“MsgHeartbeat”，并且消息的term比candidate的大，candidate回退到follower并且更新它的提交index为这次心跳里的值。然后candidate发送消息到它的信箱。  // 当消息发送到follower的Step方法，并且消息的term比follower的大，follower更新它的leader id \tMsgHeartbeat MessageType = 8 // 心跳响应；  // leader收到后就知道有哪些follower响应了。  // 只有当leader的最后提交index比follower的Match index大时，leader执行“sendAppend”方法 \tMsgHeartbeatResp MessageType = 9 // 表明请求没有被交付；  // 当“MsgUnreachable”被传送到leader的Step方法，leader发现follower无法到达，很有可能“MsgApp”都丢失了。当follower的进度状态为复制时，leader设置它回probe（哨兵） \tMsgUnreachable MessageType = 10 // 表明快照安装消息的结果  // 当一个follower拒绝了“MsgSnap”，这显示快照请求失败了--因为网络原因；**leader认为follower成为哨兵了**?(Then leader considers follower\u0026#39;s progress as probe.)；  // 当“MsgSnap”没有被拒绝，它表明快照成功了，leader设置follower的进度为哨兵，并恢复它的日志复制 \tMsgSnapStatus MessageType = 11 MsgCheckQuorum MessageType = 12 MsgTransferLeader MessageType = 13 MsgTimeoutNow MessageType = 14 MsgReadIndex MessageType = 15 MsgReadIndexResp MessageType = 16 // \u0026#34;MsgPreVote\u0026#34;和“MsgPreVoteResp”用在可选的两阶段选举协议上；  // 当Config.PreVote为true，将会进行一次预选举，除非预选举表明竞争节点会赢，否则没有节点会增加它们的term值。  // 这最小化了**一个发生了分区的节点重新加入到集群时**会带来的中断/干扰 \tMsgPreVote MessageType = 17 MsgPreVoteResp MessageType = 18 ) raft, Node and RawNode #  type Node interface { // ... } func StartNode(...) Node { rn, err := NewRawNode(...) if err != nil { panic(err) } n := newNode(rn) go n.run() return \u0026amp;n } func NewRawNode(config *Config) (*RawNode, error) { r := newRaft(config) rn := \u0026amp;RawNode{ raft: r, } ... return rn, nil } type node struct { // impl Node interface  ... rn *RawNode } func newNode(rn *RawNode) node { return node{ ... } } 实现 #  使用 #  "}]